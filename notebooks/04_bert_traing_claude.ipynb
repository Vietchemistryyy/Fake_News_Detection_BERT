{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vietchemistryyy/Fake_News_Detection_BERT/blob/main/notebooks/04_bert_traing_claude.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fb582be6",
      "metadata": {
        "id": "fb582be6",
        "outputId": "69f5acd2-8327-4fd0-b81f-4dcd9cf057be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'src'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-65766964.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODELS_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRESULTS_DIR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_claude\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_data_loaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_claude\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBERTFakeNewsClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 1: Imports and Setup\n",
        "# ============================================================================\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoTokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from src.config import ModelConfig, TrainingConfig, MODELS_DIR, RESULTS_DIR\n",
        "from src.dataset_claude import load_data_loaders\n",
        "from src.model_claude import load_model, BERTFakeNewsClassifier\n",
        "from src.train_claude import Trainer, train_model\n",
        "from src.evaluate_claude import Evaluator\n",
        "from src.utils import set_seed, Timer\n",
        "\n",
        "print(\"‚úÖ Imports successful!\")\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üñ•Ô∏è  Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d50df15",
      "metadata": {
        "id": "8d50df15"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Set Random Seed\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SETTING RANDOM SEED\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "set_seed(TrainingConfig.SEED)\n",
        "print(f\"‚úÖ Random seed set to: {TrainingConfig.SEED}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77d34e7f",
      "metadata": {
        "id": "77d34e7f"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Load Data\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LOADING DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "with Timer(\"Data loading\"):\n",
        "    train_loader, val_loader, test_loader = load_data_loaders(\n",
        "        tokenizer_name=ModelConfig.MODEL_NAME\n",
        "    )\n",
        "\n",
        "print(f\"\\n‚úÖ Data loaders created!\")\n",
        "print(f\"   Training batches:   {len(train_loader)}\")\n",
        "print(f\"   Validation batches: {len(val_loader)}\")\n",
        "print(f\"   Test batches:       {len(test_loader)}\")\n",
        "print(f\"   Batch size:         {ModelConfig.BATCH_SIZE}\")\n",
        "\n",
        "# Calculate total samples\n",
        "total_train = len(train_loader) * ModelConfig.BATCH_SIZE\n",
        "total_val = len(val_loader) * ModelConfig.BATCH_SIZE\n",
        "total_test = len(test_loader) * ModelConfig.BATCH_SIZE\n",
        "\n",
        "print(f\"\\nüìä Approximate sample counts:\")\n",
        "print(f\"   Training:   ~{total_train:,}\")\n",
        "print(f\"   Validation: ~{total_val:,}\")\n",
        "print(f\"   Test:       ~{total_test:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73d63ae4",
      "metadata": {
        "id": "73d63ae4"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Test Data Loading\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TESTING DATA LOADER\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get one batch\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "print(f\"\\nüì¶ Batch structure:\")\n",
        "print(f\"   Input IDs shape:      {batch['input_ids'].shape}\")\n",
        "print(f\"   Attention mask shape: {batch['attention_mask'].shape}\")\n",
        "print(f\"   Labels shape:         {batch['label'].shape}\")\n",
        "\n",
        "# Decode sample\n",
        "tokenizer = AutoTokenizer.from_pretrained(ModelConfig.MODEL_NAME)\n",
        "sample_text = tokenizer.decode(batch['input_ids'][0], skip_special_tokens=True)\n",
        "sample_label = batch['label'][0].item()\n",
        "\n",
        "print(f\"\\nüìù Sample from batch:\")\n",
        "print(f\"   Text: {sample_text[:200]}...\")\n",
        "print(f\"   Label: {sample_label} ({'Fake' if sample_label == 1 else 'Real'})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a902dc90",
      "metadata": {
        "id": "a902dc90"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Initialize Model\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INITIALIZING MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "with Timer(\"Model initialization\"):\n",
        "    model = load_model(\n",
        "        model_name=ModelConfig.MODEL_NAME,\n",
        "        num_labels=ModelConfig.NUM_LABELS,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "print(f\"\\n‚úÖ Model initialized!\")\n",
        "print(f\"   Model: {ModelConfig.MODEL_NAME}\")\n",
        "print(f\"   Num labels: {ModelConfig.NUM_LABELS}\")\n",
        "print(f\"   Device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00d61860",
      "metadata": {
        "id": "00d61860"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Test Forward Pass\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TESTING FORWARD PASS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Get batch\n",
        "    batch = next(iter(train_loader))\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['label'].to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    logits = model(input_ids, attention_mask)\n",
        "\n",
        "    # Get predictions\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    predictions = torch.argmax(probs, dim=1)\n",
        "\n",
        "print(f\"‚úÖ Forward pass successful!\")\n",
        "print(f\"   Logits shape: {logits.shape}\")\n",
        "print(f\"   Sample logits: {logits[0]}\")\n",
        "print(f\"   Sample probs: {probs[0]}\")\n",
        "print(f\"   Sample prediction: {predictions[0].item()}\")\n",
        "print(f\"   Sample true label: {labels[0].item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16e150fe",
      "metadata": {
        "id": "16e150fe"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Initialize Trainer\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INITIALIZING TRAINER\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    device=device,\n",
        "    learning_rate=ModelConfig.LEARNING_RATE,\n",
        "    epochs=ModelConfig.NUM_EPOCHS\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Trainer initialized!\")\n",
        "print(f\"   Optimizer: AdamW\")\n",
        "print(f\"   Learning rate: {ModelConfig.LEARNING_RATE}\")\n",
        "print(f\"   Epochs: {ModelConfig.NUM_EPOCHS}\")\n",
        "print(f\"   Warmup steps: {ModelConfig.WARMUP_STEPS}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30cc1747",
      "metadata": {
        "id": "30cc1747"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Start Training\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n‚ö†Ô∏è  This will take some time...\")\n",
        "print(f\"   Estimated time per epoch: ~{len(train_loader) * 2 / 60:.1f} minutes\")\n",
        "print(f\"   Total estimated time: ~{len(train_loader) * 2 * ModelConfig.NUM_EPOCHS / 60:.1f} minutes\")\n",
        "print(\"\\nüöÄ Training starting...\\n\")\n",
        "\n",
        "# Train model\n",
        "history = trainer.train(save_best=True, save_dir=MODELS_DIR)\n",
        "\n",
        "print(\"\\n‚úÖ Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d337b986",
      "metadata": {
        "id": "d337b986"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Plot Training History\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PLOTTING TRAINING HISTORY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Loss\n",
        "axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "axes[0, 0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
        "axes[0, 0].set_title('Loss over Epochs', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "# Accuracy\n",
        "axes[0, 1].plot(history['train_acc'], label='Train Accuracy', marker='o')\n",
        "axes[0, 1].plot(history['val_acc'], label='Val Accuracy', marker='s')\n",
        "axes[0, 1].set_title('Accuracy over Epochs', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Accuracy')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(alpha=0.3)\n",
        "\n",
        "# Learning Rate\n",
        "axes[1, 0].plot(history['learning_rates'], marker='o', color='orange')\n",
        "axes[1, 0].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Learning Rate')\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "# Comparison\n",
        "epochs = range(1, len(history['train_loss']) + 1)\n",
        "axes[1, 1].plot(epochs, history['train_acc'], label='Train Acc', marker='o', linestyle='--')\n",
        "axes[1, 1].plot(epochs, history['val_acc'], label='Val Acc', marker='s', linestyle='--')\n",
        "axes[1, 1].axhline(y=max(history['val_acc']), color='r', linestyle=':',\n",
        "                   label=f'Best Val Acc: {max(history[\"val_acc\"]):.4f}')\n",
        "axes[1, 1].set_title('Training vs Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Accuracy')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "plt.suptitle('BERT Training History', fontsize=16, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.savefig(RESULTS_DIR / 'visualizations' / 'training_history.png', dpi=300, bbox_inches='tight')\n",
        "print(\"‚úÖ Training history plot saved!\")\n",
        "plt.show()\n",
        "\n",
        "# Print summary\n",
        "print(\"\\nüìä Training Summary:\")\n",
        "print(f\"   Best validation loss: {min(history['val_loss']):.4f}\")\n",
        "print(f\"   Best validation accuracy: {max(history['val_acc']):.4f}\")\n",
        "print(f\"   Final train loss: {history['train_loss'][-1]:.4f}\")\n",
        "print(f\"   Final train accuracy: {history['train_acc'][-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a243db1",
      "metadata": {
        "id": "6a243db1"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 10: Load Best Model\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LOADING BEST MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from src.model import load_checkpoint\n",
        "\n",
        "best_model_path = MODELS_DIR / ModelConfig.BEST_MODEL_NAME\n",
        "best_model = load_checkpoint(best_model_path, device=device)\n",
        "\n",
        "print(f\"‚úÖ Best model loaded from: {best_model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37d06043",
      "metadata": {
        "id": "37d06043"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 11: Evaluate on Test Set\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATING ON TEST SET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "evaluator = Evaluator(best_model, device=device)\n",
        "test_metrics = evaluator.evaluate(test_loader, save_results=True)\n",
        "\n",
        "print(\"\\nüìä Test Set Results:\")\n",
        "print(f\"   Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
        "print(f\"   Precision: {test_metrics['precision']:.4f}\")\n",
        "print(f\"   Recall:    {test_metrics['recall']:.4f}\")\n",
        "print(f\"   F1-Score:  {test_metrics['f1_score']:.4f}\")\n",
        "print(f\"   ROC-AUC:   {test_metrics['roc_auc']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "771acd66",
      "metadata": {
        "id": "771acd66"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 12: Compare with Baseline\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARISON WITH BASELINE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load baseline results (if available)\n",
        "baseline_metrics = {\n",
        "    'accuracy': 0.85,   # Example - replace with actual baseline results\n",
        "    'precision': 0.84,\n",
        "    'recall': 0.86,\n",
        "    'f1_score': 0.85\n",
        "}\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n",
        "    'Baseline (TF-IDF + LR)': [\n",
        "        baseline_metrics['accuracy'],\n",
        "        baseline_metrics['precision'],\n",
        "        baseline_metrics['recall'],\n",
        "        baseline_metrics['f1_score'],\n",
        "        0.90  # Approximate\n",
        "    ],\n",
        "    'BERT Model': [\n",
        "        test_metrics['accuracy'],\n",
        "        test_metrics['precision'],\n",
        "        test_metrics['recall'],\n",
        "        test_metrics['f1_score'],\n",
        "        test_metrics['roc_auc']\n",
        "    ]\n",
        "})\n",
        "\n",
        "comparison_df['Improvement'] = comparison_df['BERT Model'] - comparison_df['Baseline (TF-IDF + LR)']\n",
        "comparison_df['Improvement %'] = (comparison_df['Improvement'] / comparison_df['Baseline (TF-IDF + LR)']) * 100\n",
        "\n",
        "print(\"\\nüìä Model Comparison:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualize comparison\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "x = np.arange(len(comparison_df))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax.bar(x - width/2, comparison_df['Baseline (TF-IDF + LR)'],\n",
        "              width, label='Baseline', color='skyblue')\n",
        "bars2 = ax.bar(x + width/2, comparison_df['BERT Model'],\n",
        "              width, label='BERT', color='salmon')\n",
        "\n",
        "ax.set_xlabel('Metrics', fontsize=12)\n",
        "ax.set_ylabel('Score', fontsize=12)\n",
        "ax.set_title('Baseline vs BERT Model Comparison', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(comparison_df['Metric'])\n",
        "ax.legend()\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "               f'{height:.3f}',\n",
        "               ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(RESULTS_DIR / 'visualizations' / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "print(\"\\n‚úÖ Comparison plot saved!\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d393e89d",
      "metadata": {
        "id": "d393e89d"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 13: Test Sample Predictions\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TESTING SAMPLE PREDICTIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def predict_text(text, model, tokenizer, device):\n",
        "    \"\"\"Predict single text\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=ModelConfig.MAX_LENGTH,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        prediction = torch.argmax(probs, dim=1)\n",
        "\n",
        "    return prediction.item(), probs[0].cpu().numpy()\n",
        "\n",
        "# Test samples\n",
        "test_samples = [\n",
        "    \"Breaking news: Scientists discover cure for cancer in major breakthrough\",\n",
        "    \"You won't believe what happened next! Click here for shocking truth\",\n",
        "    \"The president announced new economic policies in a press conference today\",\n",
        "    \"Aliens landed in New York and nobody noticed! Government cover-up exposed\"\n",
        "]\n",
        "\n",
        "print(\"\\nüîç Sample Predictions:\\n\")\n",
        "for i, text in enumerate(test_samples, 1):\n",
        "    pred, probs = predict_text(text, best_model, tokenizer, device)\n",
        "    print(f\"Sample {i}:\")\n",
        "    print(f\"   Text: {text}\")\n",
        "    print(f\"   Prediction: {'FAKE' if pred == 1 else 'REAL'}\")\n",
        "    print(f\"   Confidence: Real={probs[0]:.3f}, Fake={probs[1]:.3f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88bb363b",
      "metadata": {
        "id": "88bb363b"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 14: Save Final Report\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATING FINAL REPORT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "final_report = {\n",
        "    \"model_name\": ModelConfig.MODEL_NAME,\n",
        "    \"training_config\": {\n",
        "        \"epochs\": ModelConfig.NUM_EPOCHS,\n",
        "        \"batch_size\": ModelConfig.BATCH_SIZE,\n",
        "        \"learning_rate\": ModelConfig.LEARNING_RATE,\n",
        "        \"max_length\": ModelConfig.MAX_LENGTH\n",
        "    },\n",
        "    \"training_history\": {\n",
        "        \"best_val_loss\": min(history['val_loss']),\n",
        "        \"best_val_acc\": max(history['val_acc']),\n",
        "        \"final_train_loss\": history['train_loss'][-1],\n",
        "        \"final_train_acc\": history['train_acc'][-1]\n",
        "    },\n",
        "    \"test_metrics\": test_metrics,\n",
        "    \"comparison\": comparison_df.to_dict()\n",
        "}\n",
        "\n",
        "from src.utils import save_json\n",
        "report_path = RESULTS_DIR / 'metrics' / 'final_report.json'\n",
        "save_json(final_report, report_path)\n",
        "\n",
        "print(f\"‚úÖ Final report saved to: {report_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe82db2",
      "metadata": {
        "id": "dbe82db2"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 15: Summary\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ TRAINING COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüìå Summary:\")\n",
        "print(f\"   ‚úì Model trained: {ModelConfig.MODEL_NAME}\")\n",
        "print(f\"   ‚úì Best validation accuracy: {max(history['val_acc']):.4f}\")\n",
        "print(f\"   ‚úì Test accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "print(f\"   ‚úì Test F1-Score: {test_metrics['f1_score']:.4f}\")\n",
        "print(f\"   ‚úì Model saved: {MODELS_DIR / ModelConfig.BEST_MODEL_NAME}\")\n",
        "\n",
        "print(\"\\nüéØ Next Steps:\")\n",
        "print(\"   1. Deploy model to production (Phase 3)\")\n",
        "print(\"   2. Build FastAPI backend\")\n",
        "print(\"   3. Create Next.js frontend\")\n",
        "print(\"   4. Set up MongoDB database\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}