{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb582be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15992\\65766964.py\", line 16, in <module>\n",
      "    from src.config import ModelConfig, TrainingConfig, MODELS_DIR, RESULTS_DIR\n",
      "  File \"d:\\Fake_News_Detection_BERT\\notebooks\\..\\src\\__init__.py\", line 11, in <module>\n",
      "    from .model import BaselineModel\n",
      "  File \"d:\\Fake_News_Detection_BERT\\notebooks\\..\\src\\model.py\", line 18, in <module>\n",
      "    from transformers import (\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1463, in __getattr__\n",
      "    value = getattr(module, name)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1462, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1472, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py\", line 42, in <module>\n",
      "    from ...modeling_utils import PreTrainedModel\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 44, in <module>\n",
      "    from .generation import GenerationConfig, GenerationMixin\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1462, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1472, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py\", line 93, in <module>\n",
      "    from accelerate.hooks import AlignDevicesHook, add_hook_to_module\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\accelerate\\__init__.py\", line 16, in <module>\n",
      "    from .accelerator import Accelerator\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py\", line 35, in <module>\n",
      "    from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\accelerate\\checkpointing.py\", line 24, in <module>\n",
      "    from .utils import (\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\accelerate\\utils\\__init__.py\", line 178, in <module>\n",
      "    from .fsdp_utils import load_fsdp_model, load_fsdp_optimizer, save_fsdp_model, save_fsdp_optimizer\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\accelerate\\utils\\fsdp_utils.py\", line 26, in <module>\n",
      "    import torch.distributed.checkpoint as dist_cp\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\torch\\distributed\\checkpoint\\__init__.py\", line 2, in <module>\n",
      "    from .default_planner import DefaultLoadPlanner, DefaultSavePlanner\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\torch\\distributed\\checkpoint\\default_planner.py\", line 13, in <module>\n",
      "    from torch.distributed._tensor import DTensor\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\torch\\distributed\\_tensor\\__init__.py\", line 6, in <module>\n",
      "    import torch.distributed._tensor.ops\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\torch\\distributed\\_tensor\\ops\\__init__.py\", line 2, in <module>\n",
      "    from .embedding_ops import *  # noqa: F403\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\torch\\distributed\\_tensor\\ops\\embedding_ops.py\", line 8, in <module>\n",
      "    import torch.distributed._functional_collectives as funcol\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\torch\\distributed\\_functional_collectives.py\", line 12, in <module>\n",
      "    from . import _functional_collectives_impl as fun_col_impl\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\torch\\distributed\\_functional_collectives_impl.py\", line 36, in <module>\n",
      "    from torch._dynamo import assume_constant_result\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\torch\\_dynamo\\__init__.py\", line 64, in <module>\n",
      "    torch.manual_seed = disable(torch.manual_seed)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\torch\\_dynamo\\decorators.py\", line 50, in disable\n",
      "    return DisableContext()(fn)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 410, in __call__\n",
      "    (filename is None or trace_rules.check(fn))\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 3378, in check\n",
      "    return check_verbose(obj, is_inlined_call).skipped\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 3361, in check_verbose\n",
      "    rule = torch._dynamo.trace_rules.lookup_inner(\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 3442, in lookup_inner\n",
      "    rule = get_torch_obj_rule_map().get(obj, None)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 2782, in get_torch_obj_rule_map\n",
      "    obj = load_object(k)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 2811, in load_object\n",
      "    val = _load_obj_from_str(x[0])\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 2795, in _load_obj_from_str\n",
      "    return getattr(importlib.import_module(module), obj_name)\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\torch\\nested\\_internal\\nested_tensor.py\", line 417, in <module>\n",
      "    values=torch.randn(3, 3, device=\"meta\"),\n",
      "INFO:datasets:PyTorch version 2.3.1 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Transformers 4.39.3 loaded successfully\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model_claude'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset_claude\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_data_loaders\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_claude\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model, BERTFakeNewsClassifier\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrain_claude\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer, train_model\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluate_claude\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Evaluator\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_seed, Timer\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Fake_News_Detection_BERT\\notebooks\\..\\src\\train_claude.py:16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelConfig, TrainingConfig, MODELS_DIR\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodel_claude\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BERTFakeNewsClassifier, save_model\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m setup_logger, set_seed, save_json\n\u001b[32m     19\u001b[39m logger = setup_logger(\u001b[33m'\u001b[39m\u001b[33mtraining\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'model_claude'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: Imports and Setup\n",
    "# ============================================================================\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.config import ModelConfig, TrainingConfig, MODELS_DIR, RESULTS_DIR\n",
    "from src.dataset_claude import load_data_loaders\n",
    "from src.model_claude import load_model, BERTFakeNewsClassifier\n",
    "from src.train_claude import Trainer, train_model\n",
    "from src.evaluate_claude import Evaluator\n",
    "from src.utils import set_seed, Timer\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50df15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Set Random Seed\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SETTING RANDOM SEED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "set_seed(TrainingConfig.SEED)\n",
    "print(f\"‚úÖ Random seed set to: {TrainingConfig.SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d34e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Load Data\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with Timer(\"Data loading\"):\n",
    "    train_loader, val_loader, test_loader = load_data_loaders(\n",
    "        tokenizer_name=ModelConfig.MODEL_NAME\n",
    "    )\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaders created!\")\n",
    "print(f\"   Training batches:   {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "print(f\"   Test batches:       {len(test_loader)}\")\n",
    "print(f\"   Batch size:         {ModelConfig.BATCH_SIZE}\")\n",
    "\n",
    "# Calculate total samples\n",
    "total_train = len(train_loader) * ModelConfig.BATCH_SIZE\n",
    "total_val = len(val_loader) * ModelConfig.BATCH_SIZE\n",
    "total_test = len(test_loader) * ModelConfig.BATCH_SIZE\n",
    "\n",
    "print(f\"\\nüìä Approximate sample counts:\")\n",
    "print(f\"   Training:   ~{total_train:,}\")\n",
    "print(f\"   Validation: ~{total_val:,}\")\n",
    "print(f\"   Test:       ~{total_test:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d63ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Test Data Loading\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING DATA LOADER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get one batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(f\"\\nüì¶ Batch structure:\")\n",
    "print(f\"   Input IDs shape:      {batch['input_ids'].shape}\")\n",
    "print(f\"   Attention mask shape: {batch['attention_mask'].shape}\")\n",
    "print(f\"   Labels shape:         {batch['label'].shape}\")\n",
    "\n",
    "# Decode sample\n",
    "tokenizer = AutoTokenizer.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "sample_text = tokenizer.decode(batch['input_ids'][0], skip_special_tokens=True)\n",
    "sample_label = batch['label'][0].item()\n",
    "\n",
    "print(f\"\\nüìù Sample from batch:\")\n",
    "print(f\"   Text: {sample_text[:200]}...\")\n",
    "print(f\"   Label: {sample_label} ({'Fake' if sample_label == 1 else 'Real'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a902dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Initialize Model\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INITIALIZING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with Timer(\"Model initialization\"):\n",
    "    model = load_model(\n",
    "        model_name=ModelConfig.MODEL_NAME,\n",
    "        num_labels=ModelConfig.NUM_LABELS,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "print(f\"\\n‚úÖ Model initialized!\")\n",
    "print(f\"   Model: {ModelConfig.MODEL_NAME}\")\n",
    "print(f\"   Num labels: {ModelConfig.NUM_LABELS}\")\n",
    "print(f\"   Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d61860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Test Forward Pass\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING FORWARD PASS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get batch\n",
    "    batch = next(iter(train_loader))\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['label'].to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = model(input_ids, attention_mask)\n",
    "    \n",
    "    # Get predictions\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    predictions = torch.argmax(probs, dim=1)\n",
    "\n",
    "print(f\"‚úÖ Forward pass successful!\")\n",
    "print(f\"   Logits shape: {logits.shape}\")\n",
    "print(f\"   Sample logits: {logits[0]}\")\n",
    "print(f\"   Sample probs: {probs[0]}\")\n",
    "print(f\"   Sample prediction: {predictions[0].item()}\")\n",
    "print(f\"   Sample true label: {labels[0].item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e150fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: Initialize Trainer\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INITIALIZING TRAINER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    learning_rate=ModelConfig.LEARNING_RATE,\n",
    "    epochs=ModelConfig.NUM_EPOCHS\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Trainer initialized!\")\n",
    "print(f\"   Optimizer: AdamW\")\n",
    "print(f\"   Learning rate: {ModelConfig.LEARNING_RATE}\")\n",
    "print(f\"   Epochs: {ModelConfig.NUM_EPOCHS}\")\n",
    "print(f\"   Warmup steps: {ModelConfig.WARMUP_STEPS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc1747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: Start Training\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n‚ö†Ô∏è  This will take some time...\")\n",
    "print(f\"   Estimated time per epoch: ~{len(train_loader) * 2 / 60:.1f} minutes\")\n",
    "print(f\"   Total estimated time: ~{len(train_loader) * 2 * ModelConfig.NUM_EPOCHS / 60:.1f} minutes\")\n",
    "print(\"\\nüöÄ Training starting...\\n\")\n",
    "\n",
    "# Train model\n",
    "history = trainer.train(save_best=True, save_dir=MODELS_DIR)\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d337b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: Plot Training History\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PLOTTING TRAINING HISTORY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0, 0].set_title('Loss over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history['train_acc'], label='Train Accuracy', marker='o')\n",
    "axes[0, 1].plot(history['val_acc'], label='Val Accuracy', marker='s')\n",
    "axes[0, 1].set_title('Accuracy over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Learning Rate\n",
    "axes[1, 0].plot(history['learning_rates'], marker='o', color='orange')\n",
    "axes[1, 0].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Learning Rate')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Comparison\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "axes[1, 1].plot(epochs, history['train_acc'], label='Train Acc', marker='o', linestyle='--')\n",
    "axes[1, 1].plot(epochs, history['val_acc'], label='Val Acc', marker='s', linestyle='--')\n",
    "axes[1, 1].axhline(y=max(history['val_acc']), color='r', linestyle=':', \n",
    "                   label=f'Best Val Acc: {max(history[\"val_acc\"]):.4f}')\n",
    "axes[1, 1].set_title('Training vs Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('BERT Training History', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'visualizations' / 'training_history.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Training history plot saved!\")\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nüìä Training Summary:\")\n",
    "print(f\"   Best validation loss: {min(history['val_loss']):.4f}\")\n",
    "print(f\"   Best validation accuracy: {max(history['val_acc']):.4f}\")\n",
    "print(f\"   Final train loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"   Final train accuracy: {history['train_acc'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a243db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: Load Best Model\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING BEST MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from src.model import load_checkpoint\n",
    "\n",
    "best_model_path = MODELS_DIR / ModelConfig.BEST_MODEL_NAME\n",
    "best_model = load_checkpoint(best_model_path, device=device)\n",
    "\n",
    "print(f\"‚úÖ Best model loaded from: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d06043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: Evaluate on Test Set\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "evaluator = Evaluator(best_model, device=device)\n",
    "test_metrics = evaluator.evaluate(test_loader, save_results=True)\n",
    "\n",
    "print(\"\\nüìä Test Set Results:\")\n",
    "print(f\"   Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"   Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"   Recall:    {test_metrics['recall']:.4f}\")\n",
    "print(f\"   F1-Score:  {test_metrics['f1_score']:.4f}\")\n",
    "print(f\"   ROC-AUC:   {test_metrics['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771acd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 12: Compare with Baseline\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON WITH BASELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load baseline results (if available)\n",
    "baseline_metrics = {\n",
    "    'accuracy': 0.85,   # Example - replace with actual baseline results\n",
    "    'precision': 0.84,\n",
    "    'recall': 0.86,\n",
    "    'f1_score': 0.85\n",
    "}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n",
    "    'Baseline (TF-IDF + LR)': [\n",
    "        baseline_metrics['accuracy'],\n",
    "        baseline_metrics['precision'],\n",
    "        baseline_metrics['recall'],\n",
    "        baseline_metrics['f1_score'],\n",
    "        0.90  # Approximate\n",
    "    ],\n",
    "    'BERT Model': [\n",
    "        test_metrics['accuracy'],\n",
    "        test_metrics['precision'],\n",
    "        test_metrics['recall'],\n",
    "        test_metrics['f1_score'],\n",
    "        test_metrics['roc_auc']\n",
    "    ]\n",
    "})\n",
    "\n",
    "comparison_df['Improvement'] = comparison_df['BERT Model'] - comparison_df['Baseline (TF-IDF + LR)']\n",
    "comparison_df['Improvement %'] = (comparison_df['Improvement'] / comparison_df['Baseline (TF-IDF + LR)']) * 100\n",
    "\n",
    "print(\"\\nüìä Model Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, comparison_df['Baseline (TF-IDF + LR)'], \n",
    "              width, label='Baseline', color='skyblue')\n",
    "bars2 = ax.bar(x + width/2, comparison_df['BERT Model'], \n",
    "              width, label='BERT', color='salmon')\n",
    "\n",
    "ax.set_xlabel('Metrics', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Baseline vs BERT Model Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Metric'])\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.3f}',\n",
    "               ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'visualizations' / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n‚úÖ Comparison plot saved!\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d393e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 13: Test Sample Predictions\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def predict_text(text, model, tokenizer, device):\n",
    "    \"\"\"Predict single text\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=ModelConfig.MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        prediction = torch.argmax(probs, dim=1)\n",
    "    \n",
    "    return prediction.item(), probs[0].cpu().numpy()\n",
    "\n",
    "# Test samples\n",
    "test_samples = [\n",
    "    \"Breaking news: Scientists discover cure for cancer in major breakthrough\",\n",
    "    \"You won't believe what happened next! Click here for shocking truth\",\n",
    "    \"The president announced new economic policies in a press conference today\",\n",
    "    \"Aliens landed in New York and nobody noticed! Government cover-up exposed\"\n",
    "]\n",
    "\n",
    "print(\"\\nüîç Sample Predictions:\\n\")\n",
    "for i, text in enumerate(test_samples, 1):\n",
    "    pred, probs = predict_text(text, best_model, tokenizer, device)\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"   Text: {text}\")\n",
    "    print(f\"   Prediction: {'FAKE' if pred == 1 else 'REAL'}\")\n",
    "    print(f\"   Confidence: Real={probs[0]:.3f}, Fake={probs[1]:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bb363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 14: Save Final Report\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING FINAL REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_report = {\n",
    "    \"model_name\": ModelConfig.MODEL_NAME,\n",
    "    \"training_config\": {\n",
    "        \"epochs\": ModelConfig.NUM_EPOCHS,\n",
    "        \"batch_size\": ModelConfig.BATCH_SIZE,\n",
    "        \"learning_rate\": ModelConfig.LEARNING_RATE,\n",
    "        \"max_length\": ModelConfig.MAX_LENGTH\n",
    "    },\n",
    "    \"training_history\": {\n",
    "        \"best_val_loss\": min(history['val_loss']),\n",
    "        \"best_val_acc\": max(history['val_acc']),\n",
    "        \"final_train_loss\": history['train_loss'][-1],\n",
    "        \"final_train_acc\": history['train_acc'][-1]\n",
    "    },\n",
    "    \"test_metrics\": test_metrics,\n",
    "    \"comparison\": comparison_df.to_dict()\n",
    "}\n",
    "\n",
    "from src.utils import save_json\n",
    "report_path = RESULTS_DIR / 'metrics' / 'final_report.json'\n",
    "save_json(final_report, report_path)\n",
    "\n",
    "print(f\"‚úÖ Final report saved to: {report_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe82db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 15: Summary\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìå Summary:\")\n",
    "print(f\"   ‚úì Model trained: {ModelConfig.MODEL_NAME}\")\n",
    "print(f\"   ‚úì Best validation accuracy: {max(history['val_acc']):.4f}\")\n",
    "print(f\"   ‚úì Test accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"   ‚úì Test F1-Score: {test_metrics['f1_score']:.4f}\")\n",
    "print(f\"   ‚úì Model saved: {MODELS_DIR / ModelConfig.BEST_MODEL_NAME}\")\n",
    "\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"   1. Deploy model to production (Phase 3)\")\n",
    "print(\"   2. Build FastAPI backend\")\n",
    "print(\"   3. Create Next.js frontend\")\n",
    "print(\"   4. Set up MongoDB database\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
