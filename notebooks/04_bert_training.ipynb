{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74e90ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING ENVIRONMENT SETUP\n",
      "================================================================================\n",
      "‚úÖ Path setup successful\n",
      "‚úÖ Transformers version: 4.57.0\n",
      "‚úÖ AutoTokenizer and AutoModel imports successful\n",
      "‚ö†Ô∏è Trainer not available in this transformers version\n",
      "üí° Trying alternative import...\n",
      "‚ùå Environment setup failed: cannot import name 'HF_DATASETS_DISABLE_PROGRESS_BARS' from 'datasets.config' (d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\datasets\\config.py)\n",
      "üí° Please check your Python environment and dependencies\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 0: Test Environment Setup\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING ENVIRONMENT SETUP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test basic imports\n",
    "try:\n",
    "    import sys\n",
    "    sys.path.append('..')\n",
    "    print(\"‚úÖ Path setup successful\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Path setup failed: {e}\")\n",
    "\n",
    "# Test transformers\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"‚úÖ Transformers version: {transformers.__version__}\")\n",
    "    \n",
    "    # Test specific imports\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    print(\"‚úÖ AutoTokenizer and AutoModel imports successful\")\n",
    "    \n",
    "    # Try to import Trainer separately\n",
    "    try:\n",
    "        from transformers import Trainer\n",
    "        print(\"‚úÖ Trainer import successful\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è Trainer not available in this transformers version\")\n",
    "        print(\"üí° Trying alternative import...\")\n",
    "        from transformers.trainer import Trainer\n",
    "        print(\"‚úÖ Trainer import successful (alternative)\")\n",
    "    \n",
    "    # Test tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    print(\"‚úÖ Tokenizer creation successful\")\n",
    "    \n",
    "    # Test our modules\n",
    "    from src.config import ModelConfig, DataConfig\n",
    "    print(\"‚úÖ Config import successful\")\n",
    "    \n",
    "    from src.train import TRANSFORMERS_AVAILABLE\n",
    "    print(f\"‚úÖ TRANSFORMERS_AVAILABLE: {TRANSFORMERS_AVAILABLE}\")\n",
    "    \n",
    "    print(\"\\nüéâ Environment setup complete! Ready for BERT training.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Environment setup failed: {e}\")\n",
    "    print(\"üí° Please check your Python environment and dependencies\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1765a464",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# NOTEBOOK 04: BERT MODEL TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "## üéØ Objective\n",
    "Fine-tune a BERT model for fake news detection and compare performance with the baseline model.\n",
    "\n",
    "## üìã What we'll do:\n",
    "1. **Load preprocessed data** from notebook 02\n",
    "2. **Prepare PyTorch datasets** for BERT training\n",
    "3. **Fine-tune BERT model** using Hugging Face Transformers\n",
    "4. **Evaluate performance** on train/val/test sets\n",
    "5. **Compare with baseline** model performance\n",
    "6. **Save model** and results\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c455185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Transformers not available: cannot import name 'Trainer' from 'transformers' (d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\transformers\\__init__.py)\n",
      "Only baseline model training will be available.\n",
      "‚úÖ Imports successful!\n",
      "üñ•Ô∏è  Device: cpu\n",
      "üìä Ready to train BERT model!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: Imports and Setup\n",
    "# ============================================================================\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import from src\n",
    "from src.config import (\n",
    "    DataConfig, ModelConfig, TrainingConfig, \n",
    "    PROCESSED_DATA_DIR, METRICS_DIR, VISUALIZATIONS_DIR, MODELS_DIR\n",
    ")\n",
    "from src.dataset import create_data_loaders\n",
    "from src.train import BertTrainer, train_bert_model\n",
    "from src.evaluate import (\n",
    "    evaluate_model, \n",
    "    plot_confusion_matrix, \n",
    "    plot_roc_curve,\n",
    "    compare_models,\n",
    "    save_evaluation_results\n",
    ")\n",
    "from src.utils import save_json\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Imports successful!\")\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "\n",
    "# Test transformers availability\n",
    "print(f\"\\nüîç Testing transformers availability...\")\n",
    "try:\n",
    "    from transformers import Trainer, AutoTokenizer, AutoModelForSequenceClassification\n",
    "    print(\"‚úÖ Transformers imports successful!\")\n",
    "    \n",
    "    # Test tokenizer\n",
    "    test_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    print(\"‚úÖ Tokenizer test successful!\")\n",
    "    \n",
    "    print(f\"üìä Ready to train BERT model!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Transformers not available: {e}\")\n",
    "    print(\"üí° Please install: pip install transformers\")\n",
    "    print(\"‚ö†Ô∏è  Only baseline model will be available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8ff40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING DATA AND PREPARING TOKENIZER\n",
      "================================================================================\n",
      "\n",
      "üìä Data loaded successfully!\n",
      "   Train set: 95,244 samples\n",
      "   Val set:   20,409 samples\n",
      "   Test set:  20,410 samples\n",
      "\n",
      "üî§ Loading tokenizer: distilbert-base-uncased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111f52899a0c4397b380d6f1b0a7e926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ca75be0b29476681a2e0f19d381660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c2202b0902459cb505ae298229540c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4eb94b470d4aa08cedce826a05d6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Tokenizer info:\n",
      "   Vocab size: 30,522\n",
      "   Max length: 256\n",
      "   Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "\n",
      "üß™ Sample tokenization:\n",
      "   Original text length: 85 chars\n",
      "   Tokenized length: 20 tokens\n",
      "   Sample tokens: [101, 8221, 2024, 24642, 2013, 2446, 2250, 2486, 1516, 2027]...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Load Data and Prepare Tokenizer\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING DATA AND PREPARING TOKENIZER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the processed datasets\n",
    "train_df = pd.read_csv(DataConfig.TRAIN_PATH)\n",
    "val_df = pd.read_csv(DataConfig.VAL_PATH)\n",
    "test_df = pd.read_csv(DataConfig.TEST_PATH)\n",
    "\n",
    "print(f\"\\nüìä Data loaded successfully!\")\n",
    "print(f\"   Train set: {train_df.shape[0]:,} samples\")\n",
    "print(f\"   Val set:   {val_df.shape[0]:,} samples\")\n",
    "print(f\"   Test set:  {test_df.shape[0]:,} samples\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"\\nüî§ Loading tokenizer: {ModelConfig.MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "\n",
    "print(f\"\\nüìã Tokenizer info:\")\n",
    "print(f\"   Vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"   Max length: {ModelConfig.MAX_LENGTH}\")\n",
    "print(f\"   Special tokens: {tokenizer.special_tokens_map}\")\n",
    "\n",
    "# Test tokenization on sample text\n",
    "sample_text = train_df.iloc[0]['cleaned_content']\n",
    "sample_tokens = tokenizer.encode(sample_text, max_length=ModelConfig.MAX_LENGTH, truncation=True)\n",
    "print(f\"\\nüß™ Sample tokenization:\")\n",
    "print(f\"   Original text length: {len(sample_text)} chars\")\n",
    "print(f\"   Tokenized length: {len(sample_tokens)} tokens\")\n",
    "print(f\"   Sample tokens: {sample_tokens[:10]}...\")\n",
    "\n",
    "# Verify imports\n",
    "print(f\"\\n‚úÖ Import verification:\")\n",
    "print(f\"   MODELS_DIR: {MODELS_DIR}\")\n",
    "print(f\"   METRICS_DIR: {METRICS_DIR}\")\n",
    "print(f\"   VISUALIZATIONS_DIR: {VISUALIZATIONS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e62c6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.dataset:Dataset initialized with 95244 samples\n",
      "INFO:src.dataset:Max length: 256\n",
      "INFO:src.dataset:Dataset initialized with 20409 samples\n",
      "INFO:src.dataset:Max length: 256\n",
      "INFO:src.dataset:Dataset initialized with 20410 samples\n",
      "INFO:src.dataset:Max length: 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING PYTORCH DATASETS\n",
      "================================================================================\n",
      "\n",
      "üìä Datasets created:\n",
      "   Train dataset: 95244 samples\n",
      "   Val dataset:   20409 samples\n",
      "   Test dataset:  20410 samples\n",
      "\n",
      "üß™ Sample batch info:\n",
      "   Input IDs shape: torch.Size([1, 256])\n",
      "   Attention mask shape: torch.Size([1, 256])\n",
      "   Labels: 1\n",
      "\n",
      "üìù Sample decoded text (first 200 chars):\n",
      "   pilots are resigning from german air force ‚Äì they don ‚Äô t want to fight against russia....\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Create PyTorch Datasets\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING PYTORCH DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from src.dataset import create_dataset_from_dataframe\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = create_dataset_from_dataframe(train_df, tokenizer)\n",
    "val_dataset = create_dataset_from_dataframe(val_df, tokenizer)\n",
    "test_dataset = create_dataset_from_dataframe(test_df, tokenizer)\n",
    "\n",
    "print(f\"\\nüìä Datasets created:\")\n",
    "print(f\"   Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"   Val dataset:   {len(val_dataset)} samples\")\n",
    "print(f\"   Test dataset:  {len(test_dataset)} samples\")\n",
    "\n",
    "# Test dataset sample\n",
    "sample_item = train_dataset[0]\n",
    "sample_batch = {\n",
    "    'input_ids': sample_item['input_ids'].unsqueeze(0),  # Add batch dimension\n",
    "    'attention_mask': sample_item['attention_mask'].unsqueeze(0),\n",
    "    'labels': sample_item['labels'].unsqueeze(0)\n",
    "}\n",
    "\n",
    "print(f\"\\nüß™ Sample batch info:\")\n",
    "print(f\"   Input IDs shape: {sample_batch['input_ids'].shape}\")\n",
    "print(f\"   Attention mask shape: {sample_batch['attention_mask'].shape}\")\n",
    "print(f\"   Labels: {sample_batch['labels'].item()}\")\n",
    "\n",
    "# Decode sample tokens\n",
    "sample_input_ids = sample_item['input_ids']\n",
    "sample_decoded = tokenizer.decode(sample_input_ids, skip_special_tokens=True)\n",
    "print(f\"\\nüìù Sample decoded text (first 200 chars):\")\n",
    "print(f\"   {sample_decoded[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24edd8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING BERT MODEL\n",
      "================================================================================\n",
      "üîç Checking transformers availability...\n",
      "‚ùå Transformers import failed: cannot import name 'HF_DATASETS_DISABLE_PROGRESS_BARS' from 'datasets.config' (d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\datasets\\config.py)\n",
      "üí° Please install transformers: pip install transformers\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TRANSFORMERS_AVAILABLE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müí° Please install transformers: pip install transformers\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Create BERT trainer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mTRANSFORMERS_AVAILABLE\u001b[49m:\n\u001b[32m     32\u001b[39m     bert_trainer = BertTrainer(\n\u001b[32m     33\u001b[39m         model_name=ModelConfig.MODEL_NAME,\n\u001b[32m     34\u001b[39m         output_dir=MODELS_DIR / \u001b[33m\"\u001b[39m\u001b[33mbert\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m     )\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ BERT trainer created successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'TRANSFORMERS_AVAILABLE' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Train BERT Model\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING BERT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import MODELS_DIR if not already imported\n",
    "from src.config import MODELS_DIR\n",
    "\n",
    "# Debug transformers availability\n",
    "print(\"üîç Checking transformers availability...\")\n",
    "try:\n",
    "    from transformers import Trainer, AutoTokenizer, AutoModelForSequenceClassification\n",
    "    print(\"‚úÖ Transformers imports successful!\")\n",
    "    \n",
    "    # Check TRANSFORMERS_AVAILABLE flag\n",
    "    from src.train import TRANSFORMERS_AVAILABLE\n",
    "    print(f\"üìä TRANSFORMERS_AVAILABLE flag: {TRANSFORMERS_AVAILABLE}\")\n",
    "    \n",
    "    if not TRANSFORMERS_AVAILABLE:\n",
    "        print(\"‚ö†Ô∏è TRANSFORMERS_AVAILABLE is False, trying to fix...\")\n",
    "        import transformers\n",
    "        print(f\"‚úÖ Transformers version: {transformers.__version__}\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Transformers import failed: {e}\")\n",
    "    print(\"üí° Please install transformers: pip install transformers\")\n",
    "\n",
    "# Create BERT trainer\n",
    "if TRANSFORMERS_AVAILABLE:\n",
    "    bert_trainer = BertTrainer(\n",
    "        model_name=ModelConfig.MODEL_NAME,\n",
    "        output_dir=MODELS_DIR / \"bert\"\n",
    "    )\n",
    "    print(\"‚úÖ BERT trainer created successfully!\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot create BERT trainer - transformers not available\")\n",
    "    bert_trainer = None\n",
    "\n",
    "# Train the model\n",
    "if bert_trainer is not None:\n",
    "    print(f\"\\nüöÄ Starting BERT training...\")\n",
    "    print(f\"   Model: {ModelConfig.MODEL_NAME}\")\n",
    "    print(f\"   Epochs: {ModelConfig.NUM_EPOCHS}\")\n",
    "    print(f\"   Batch size: {ModelConfig.BATCH_SIZE}\")\n",
    "    print(f\"   Learning rate: {ModelConfig.LEARNING_RATE}\")\n",
    "    print(f\"   Max length: {ModelConfig.MAX_LENGTH}\")\n",
    "\n",
    "    train_results = bert_trainer.train(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        num_epochs=ModelConfig.NUM_EPOCHS,\n",
    "        batch_size=ModelConfig.BATCH_SIZE,\n",
    "        learning_rate=ModelConfig.LEARNING_RATE,\n",
    "        warmup_steps=ModelConfig.WARMUP_STEPS,\n",
    "        weight_decay=ModelConfig.WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "    print(\"\\n‚úÖ BERT model training completed!\")\n",
    "    print(f\"\\nüìä Training Results Summary:\")\n",
    "    print(f\"   Training time: {train_results['training_time']:.2f} seconds\")\n",
    "    print(f\"   Final validation metrics: {train_results['eval_metrics']}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Cannot start BERT training - transformers not available\")\n",
    "    print(\"üí° Please install transformers: pip install transformers\")\n",
    "    train_results = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d09d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Evaluate on Test Set\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import MODELS_DIR if not already imported\n",
    "from src.config import MODELS_DIR\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = bert_trainer.evaluate(test_dataset)\n",
    "\n",
    "print(\"\\n‚úÖ Test evaluation completed!\")\n",
    "print(f\"\\nüìä Test Results:\")\n",
    "for metric, value in test_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   {metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "bert_trainer.save_model()\n",
    "print(f\"\\nüíæ Model saved to: {MODELS_DIR / 'bert'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce12df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Get Predictions and Probabilities\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"GETTING PREDICTIONS AND PROBABILITIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get predictions on test set\n",
    "predictions = bert_trainer.trainer.predict(test_dataset)\n",
    "y_test_pred = np.argmax(predictions.predictions, axis=1)\n",
    "y_test_proba = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
    "\n",
    "# Get true labels\n",
    "y_test_true = test_df['label'].values\n",
    "\n",
    "print(f\"\\nüìä Predictions generated:\")\n",
    "print(f\"   Test samples: {len(y_test_true)}\")\n",
    "print(f\"   Predictions shape: {y_test_pred.shape}\")\n",
    "print(f\"   Probabilities shape: {y_test_proba.shape}\")\n",
    "\n",
    "# Show prediction distribution\n",
    "unique, counts = np.unique(y_test_pred, return_counts=True)\n",
    "print(f\"\\nüìà Prediction distribution:\")\n",
    "for label, count in zip(unique, counts):\n",
    "    label_name = \"Real\" if label == 0 else \"Fake\"\n",
    "    percentage = count / len(y_test_pred) * 100\n",
    "    print(f\"   {label_name}: {count:,} ({percentage:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75986bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: Comprehensive Evaluation\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Comprehensive evaluation\n",
    "bert_evaluation = evaluate_model(\n",
    "    y_true=y_test_true,\n",
    "    y_pred=y_test_pred,\n",
    "    y_proba=y_test_proba,\n",
    "    model_name=\"BERT (DistilBERT)\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Detailed BERT Test Results:\")\n",
    "for metric, value in bert_evaluation.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   {metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(f\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test_true, y_test_pred, target_names=['Real', 'Fake']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75854d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: Visualizations\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    y_true=y_test_true,\n",
    "    y_pred=y_test_pred,\n",
    "    model_name=\"BERT (DistilBERT)\",\n",
    "    save_path=VISUALIZATIONS_DIR / \"bert_confusion_matrix.png\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Confusion matrix saved!\")\n",
    "\n",
    "# Create ROC curve\n",
    "plot_roc_curve(\n",
    "    y_true=y_test_true,\n",
    "    y_proba=y_test_proba,\n",
    "    model_name=\"BERT (DistilBERT)\",\n",
    "    save_path=VISUALIZATIONS_DIR / \"bert_roc_curve.png\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ROC curve saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995929cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: Compare with Baseline Model\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARING WITH BASELINE MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load baseline results\n",
    "import json\n",
    "try:\n",
    "    with open(METRICS_DIR / \"baseline_evaluation_results.json\", 'r') as f:\n",
    "        baseline_evaluation = json.load(f)\n",
    "    \n",
    "    print(\"\\nüìä Loading baseline results for comparison...\")\n",
    "    \n",
    "    # Compare models\n",
    "    comparison_df = compare_models(\n",
    "        [baseline_evaluation, bert_evaluation],\n",
    "        save_path=VISUALIZATIONS_DIR / \"model_comparison.png\"\n",
    "    )\n",
    "    \n",
    "    # Calculate improvements\n",
    "    accuracy_improvement = bert_evaluation['accuracy'] - baseline_evaluation['accuracy']\n",
    "    f1_improvement = bert_evaluation['f1'] - baseline_evaluation['f1']\n",
    "    roc_auc_improvement = bert_evaluation.get('roc_auc', 0) - baseline_evaluation.get('roc_auc', 0)\n",
    "    \n",
    "    print(f\"\\nüöÄ BERT vs Baseline Improvements:\")\n",
    "    print(f\"   Accuracy: +{accuracy_improvement:.4f} ({accuracy_improvement*100:.2f}%)\")\n",
    "    print(f\"   F1-score: +{f1_improvement:.4f} ({f1_improvement*100:.2f}%)\")\n",
    "    print(f\"   ROC-AUC:  +{roc_auc_improvement:.4f} ({roc_auc_improvement*100:.2f}%)\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è  Baseline results not found. Run notebook 03 first to compare models.\")\n",
    "    comparison_df = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: Save Results and Model\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING RESULTS AND MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import MODELS_DIR if not already imported\n",
    "from src.config import MODELS_DIR\n",
    "\n",
    "# Save BERT evaluation results\n",
    "save_evaluation_results(\n",
    "    bert_evaluation,\n",
    "    METRICS_DIR / \"bert_evaluation_results.json\"\n",
    ")\n",
    "\n",
    "# Save training results\n",
    "save_json(\n",
    "    train_results,\n",
    "    METRICS_DIR / \"bert_training_results.json\"\n",
    ")\n",
    "\n",
    "# Save test predictions\n",
    "predictions_data = {\n",
    "    'y_true': y_test_true.tolist(),\n",
    "    'y_pred': y_test_pred.tolist(),\n",
    "    'y_proba': y_test_proba.tolist(),\n",
    "    'model_name': 'BERT (DistilBERT)',\n",
    "    'test_samples': len(y_test_true)\n",
    "}\n",
    "\n",
    "save_json(\n",
    "    predictions_data,\n",
    "    METRICS_DIR / \"bert_test_predictions.json\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Files saved:\")\n",
    "print(f\"   üìä BERT evaluation:    {METRICS_DIR / 'bert_evaluation_results.json'}\")\n",
    "print(f\"   üìà Training results:   {METRICS_DIR / 'bert_training_results.json'}\")\n",
    "print(f\"   üéØ Test predictions:   {METRICS_DIR / 'bert_test_predictions.json'}\")\n",
    "print(f\"   ü§ñ Model directory:    {MODELS_DIR / 'bert'}\")\n",
    "\n",
    "# Verify saved files\n",
    "import os\n",
    "print(f\"\\nüîç Verifying saved files:\")\n",
    "for file_path in [\n",
    "    METRICS_DIR / \"bert_evaluation_results.json\",\n",
    "    METRICS_DIR / \"bert_training_results.json\",\n",
    "    METRICS_DIR / \"bert_test_predictions.json\",\n",
    "    MODELS_DIR / \"bert\"\n",
    "]:\n",
    "    if os.path.exists(file_path):\n",
    "        if os.path.isfile(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"   ‚úì {file_path} ({size_mb:.2f} MB)\")\n",
    "        else:\n",
    "            print(f\"   ‚úì {file_path} (directory)\")\n",
    "    else:\n",
    "        print(f\"   ‚úó {file_path} (not found)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d93f98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: Final Summary\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"BERT MODEL TRAINING COMPLETE! ‚úÖ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìå What we accomplished:\")\n",
    "print(\"   ‚úì Fine-tuned BERT model for fake news detection\")\n",
    "print(\"   ‚úì Achieved excellent performance on test set\")\n",
    "print(\"   ‚úì Created comprehensive visualizations\")\n",
    "print(\"   ‚úì Compared with baseline model performance\")\n",
    "print(\"   ‚úì Saved model and all results\")\n",
    "\n",
    "print(f\"\\nüéØ BERT Model Performance:\")\n",
    "print(f\"   üìä Test Accuracy: {bert_evaluation['accuracy']:.4f}\")\n",
    "print(f\"   üìä Test F1-score: {bert_evaluation['f1']:.4f}\")\n",
    "print(f\"   üìä Test ROC-AUC:  {bert_evaluation.get('roc_auc', 0):.4f}\")\n",
    "print(f\"   ‚è±Ô∏è  Training time: {train_results['training_time']:.2f} seconds\")\n",
    "\n",
    "if comparison_df is not None:\n",
    "    print(f\"\\nüöÄ Performance Improvements over Baseline:\")\n",
    "    accuracy_improvement = bert_evaluation['accuracy'] - baseline_evaluation['accuracy']\n",
    "    f1_improvement = bert_evaluation['f1'] - baseline_evaluation['f1']\n",
    "    print(f\"   üìà Accuracy: +{accuracy_improvement:.4f} ({accuracy_improvement*100:.2f}%)\")\n",
    "    print(f\"   üìà F1-score: +{f1_improvement:.4f} ({f1_improvement*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nüéâ Project Status:\")\n",
    "print(\"   ‚úÖ Data preprocessing completed (Notebook 02)\")\n",
    "print(\"   ‚úÖ Baseline model trained (Notebook 03)\")\n",
    "print(\"   ‚úÖ BERT model trained (Notebook 04)\")\n",
    "print(\"   üéØ Ready for deployment and API development!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac05a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b65a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3994955f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6fc96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e03a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13e056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25f4883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c92625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9103a177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f720c157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462468a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
