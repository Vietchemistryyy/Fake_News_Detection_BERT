{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e90ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING ENVIRONMENT SETUP\n",
      "================================================================================\n",
      "✅ Path setup successful\n",
      "✅ Transformers version: 4.39.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10272\\1937257539.py\", line 18, in <module>\n",
      "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1462, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1472, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\transformers\\trainer.py\", line 41, in <module>\n",
      "    from .integrations import (\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1462, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1472, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py\", line 72, in <module>\n",
      "    from ..trainer_callback import ProgressCallback, TrainerCallback  # noqa: E402\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\transformers\\trainer_callback.py\", line 27, in <module>\n",
      "    from .training_args import TrainingArguments\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\transformers\\training_args.py\", line 72, in <module>\n",
      "    from accelerate.state import AcceleratorState, PartialState\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\accelerate\\__init__.py\", line 16, in <module>\n",
      "    from .accelerator import Accelerator\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\accelerate\\accelerator.py\", line 35, in <module>\n",
      "    from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\accelerate\\checkpointing.py\", line 24, in <module>\n",
      "    from .utils import (\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\accelerate\\utils\\__init__.py\", line 178, in <module>\n",
      "    from .fsdp_utils import load_fsdp_model, load_fsdp_optimizer, save_fsdp_model, save_fsdp_optimizer\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\accelerate\\utils\\fsdp_utils.py\", line 26, in <module>\n",
      "    import torch.distributed.checkpoint as dist_cp\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\distributed\\checkpoint\\__init__.py\", line 2, in <module>\n",
      "    from .default_planner import DefaultLoadPlanner, DefaultSavePlanner\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\distributed\\checkpoint\\default_planner.py\", line 13, in <module>\n",
      "    from torch.distributed._tensor import DTensor\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\distributed\\_tensor\\__init__.py\", line 6, in <module>\n",
      "    import torch.distributed._tensor.ops\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\distributed\\_tensor\\ops\\__init__.py\", line 2, in <module>\n",
      "    from .embedding_ops import *  # noqa: F403\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\distributed\\_tensor\\ops\\embedding_ops.py\", line 8, in <module>\n",
      "    import torch.distributed._functional_collectives as funcol\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\distributed\\_functional_collectives.py\", line 12, in <module>\n",
      "    from . import _functional_collectives_impl as fun_col_impl\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\distributed\\_functional_collectives_impl.py\", line 36, in <module>\n",
      "    from torch._dynamo import assume_constant_result\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\_dynamo\\__init__.py\", line 64, in <module>\n",
      "    torch.manual_seed = disable(torch.manual_seed)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\_dynamo\\decorators.py\", line 50, in disable\n",
      "    return DisableContext()(fn)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 410, in __call__\n",
      "    (filename is None or trace_rules.check(fn))\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 3378, in check\n",
      "    return check_verbose(obj, is_inlined_call).skipped\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 3361, in check_verbose\n",
      "    rule = torch._dynamo.trace_rules.lookup_inner(\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 3442, in lookup_inner\n",
      "    rule = get_torch_obj_rule_map().get(obj, None)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 2782, in get_torch_obj_rule_map\n",
      "    obj = load_object(k)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 2811, in load_object\n",
      "    val = _load_obj_from_str(x[0])\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 2795, in _load_obj_from_str\n",
      "    return getattr(importlib.import_module(module), obj_name)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\nested\\_internal\\nested_tensor.py\", line 417, in <module>\n",
      "    values=torch.randn(3, 3, device=\"meta\"),\n",
      "d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\nested\\_internal\\nested_tensor.py:417: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  values=torch.randn(3, 3, device=\"meta\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Core transformers imports successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenizer test successful\n",
      "Transformers 4.39.3 loaded successfully\n",
      "✅ Config import successful\n",
      "❌ Environment setup failed: cannot import name 'EARLY_STOPPING_AVAILABLE' from 'src.train' (d:\\Fake_News_Detection_BERT\\notebooks\\..\\src\\train.py)\n",
      "💡 Installing transformers...\n",
      "✅ Transformers installed. Please restart kernel.\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Environment Test & Setup\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"ENVIRONMENT SETUP & VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Test basic imports\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"✅ Transformers version: {transformers.__version__}\")\n",
    "    \n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "    print(\"✅ Core transformers imports successful\")\n",
    "    \n",
    "    # Test tokenizer\n",
    "    test_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    print(\"✅ Tokenizer test successful\")\n",
    "    \n",
    "    # Import project modules\n",
    "    from src.config import ModelConfig, DataConfig\n",
    "    from src.train import TRANSFORMERS_AVAILABLE, EARLY_STOPPING_AVAILABLE\n",
    "    \n",
    "    print(f\"✅ TRANSFORMERS_AVAILABLE: {TRANSFORMERS_AVAILABLE}\")\n",
    "    print(f\"✅ EARLY_STOPPING_AVAILABLE: {EARLY_STOPPING_AVAILABLE}\")\n",
    "    print(\"\\n🎉 Environment ready for training!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"\\n💡 Installing required packages...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', \n",
    "                          'transformers', 'accelerate', 'datasets'])\n",
    "    print(\"✅ Packages installed. Please restart kernel and run again.\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1765a464",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# NOTEBOOK 04: BERT MODEL TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "## 🎯 Objective\n",
    "Fine-tune a BERT model for fake news detection and compare performance with the baseline model.\n",
    "\n",
    "## 📋 What we'll do:\n",
    "1. **Load preprocessed data** from notebook 02\n",
    "2. **Prepare PyTorch datasets** for BERT training\n",
    "3. **Fine-tune BERT model** using Hugging Face Transformers\n",
    "4. **Evaluate performance** on train/val/test sets\n",
    "5. **Compare with baseline** model performance\n",
    "6. **Save model** and results\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c455185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports successful!\n",
      "🖥️  Device: cpu\n",
      "\n",
      "🔍 Testing transformers availability...\n",
      "✅ Transformers imports successful!\n",
      "✅ Tokenizer test successful!\n",
      "📊 Ready to train BERT model!\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Complete Imports\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Import all required modules\n",
    "from src.config import (\n",
    "    DataConfig, ModelConfig, TrainingConfig, \n",
    "    PROCESSED_DATA_DIR, METRICS_DIR, VISUALIZATIONS_DIR, MODELS_DIR\n",
    ")\n",
    "from src.dataset import create_dataset_from_dataframe\n",
    "from src.train import BertTrainer, save_training_results\n",
    "from src.evaluate import (\n",
    "    compute_extended_metrics,\n",
    "    plot_confusion_matrix, \n",
    "    plot_roc_curve,\n",
    "    compare_models,\n",
    "    save_evaluation_results\n",
    ")\n",
    "from src.utils import save_json\n",
    "\n",
    "# Setup\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"IMPORTS COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"🖥️  Device: {device}\")\n",
    "print(f\"🤖 Model: {ModelConfig.MODEL_NAME}\")\n",
    "print(f\"📊 Ready to train!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8ff40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING DATA AND PREPARING TOKENIZER\n",
      "================================================================================\n",
      "\n",
      "📊 Data loaded successfully!\n",
      "   Train set: 95,244 samples\n",
      "   Val set:   20,409 samples\n",
      "   Test set:  20,410 samples\n",
      "\n",
      "🔤 Loading tokenizer: roberta-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022ba46c4f7a4a4dacc1a4bf27abe2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81b88aa03514a4bbf27e5f0fed7dd15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3418c7f631cf408eacd1a903f1e7e538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f59177619541d5aed920befeb59d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea05d91fba75423a870e4306467aa1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Tokenizer info:\n",
      "   Vocab size: 50,265\n",
      "   Max length: 256\n",
      "   Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}\n",
      "\n",
      "🧪 Sample tokenization:\n",
      "   Original text length: 85 chars\n",
      "   Tokenized length: 26 tokens\n",
      "   Sample tokens: [0, 642, 718, 5992, 32, 6749, 154, 31, 821, 7043]...\n",
      "\n",
      "✅ Import verification:\n",
      "   MODELS_DIR: d:\\Fake_News_Detection_BERT\\notebooks\\..\\models\n",
      "   METRICS_DIR: d:\\Fake_News_Detection_BERT\\notebooks\\..\\results\\metrics\n",
      "   VISUALIZATIONS_DIR: d:\\Fake_News_Detection_BERT\\notebooks\\..\\results\\visualizations\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: Load Data\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING PREPROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(DataConfig.TRAIN_PATH)\n",
    "val_df = pd.read_csv(DataConfig.VAL_PATH)\n",
    "test_df = pd.read_csv(DataConfig.TEST_PATH)\n",
    "\n",
    "print(f\"\\n📊 Data loaded successfully:\")\n",
    "print(f\"   Train: {train_df.shape[0]:>7,} samples\")\n",
    "print(f\"   Val:   {val_df.shape[0]:>7,} samples\")\n",
    "print(f\"   Test:  {test_df.shape[0]:>7,} samples\")\n",
    "print(f\"   Total: {(train_df.shape[0] + val_df.shape[0] + test_df.shape[0]):>7,} samples\")\n",
    "\n",
    "# Check label distribution\n",
    "print(f\"\\n📈 Label distribution:\")\n",
    "for name, df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "    fake_pct = (df['label'] == 1).sum() / len(df) * 100\n",
    "    print(f\"   {name:5} - Fake: {fake_pct:5.2f}%, Real: {100-fake_pct:5.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e62c6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.dataset:Dataset initialized with 95244 samples\n",
      "INFO:src.dataset:Max length: 256\n",
      "INFO:src.dataset:Dataset initialized with 20409 samples\n",
      "INFO:src.dataset:Max length: 256\n",
      "INFO:src.dataset:Dataset initialized with 20410 samples\n",
      "INFO:src.dataset:Max length: 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING PYTORCH DATASETS\n",
      "================================================================================\n",
      "\n",
      "📊 Datasets created:\n",
      "   Train dataset: 95244 samples\n",
      "   Val dataset:   20409 samples\n",
      "   Test dataset:  20410 samples\n",
      "\n",
      "🧪 Sample batch info:\n",
      "   Input IDs shape: torch.Size([1, 256])\n",
      "   Attention mask shape: torch.Size([1, 256])\n",
      "   Labels: 1\n",
      "\n",
      "📝 Sample decoded text (first 200 chars):\n",
      "   pilots are resigning from german air force – they don ’ t want to fight against russia....\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: Load Tokenizer\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING TOKENIZER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "\n",
    "print(f\"\\n🔤 Tokenizer: {ModelConfig.MODEL_NAME}\")\n",
    "print(f\"   Vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"   Max length: {ModelConfig.MAX_LENGTH}\")\n",
    "print(f\"   Padding: {ModelConfig.PADDING}\")\n",
    "print(f\"   Truncation: {ModelConfig.TRUNCATION}\")\n",
    "\n",
    "# Test tokenization\n",
    "sample_text = train_df.iloc[0]['cleaned_content']\n",
    "sample_tokens = tokenizer.encode(sample_text, max_length=ModelConfig.MAX_LENGTH, truncation=True)\n",
    "\n",
    "print(f\"\\n🧪 Tokenization test:\")\n",
    "print(f\"   Input length: {len(sample_text)} chars\")\n",
    "print(f\"   Output length: {len(sample_tokens)} tokens\")\n",
    "print(f\"   First 10 tokens: {sample_tokens[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24edd8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING BERT MODEL\n",
      "================================================================================\n",
      "🔍 Checking transformers availability...\n",
      "❌ Transformers import failed: cannot import name 'HF_DATASETS_DISABLE_PROGRESS_BARS' from 'datasets.config' (d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\datasets\\config.py)\n",
      "💡 Please install transformers: pip install transformers\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TRANSFORMERS_AVAILABLE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m💡 Please install transformers: pip install transformers\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Create BERT trainer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mTRANSFORMERS_AVAILABLE\u001b[49m:\n\u001b[32m     32\u001b[39m     bert_trainer = BertTrainer(\n\u001b[32m     33\u001b[39m         model_name=ModelConfig.MODEL_NAME,\n\u001b[32m     34\u001b[39m         output_dir=MODELS_DIR / \u001b[33m\"\u001b[39m\u001b[33mbert\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m     )\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ BERT trainer created successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'TRANSFORMERS_AVAILABLE' is not defined"
     ]
    }
   ],
   "source": [
    "# CELL 5: Create PyTorch Datasets\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING PYTORCH DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create datasets with proper error handling\n",
    "try:\n",
    "    train_dataset = create_dataset_from_dataframe(train_df, tokenizer)\n",
    "    val_dataset = create_dataset_from_dataframe(val_df, tokenizer)\n",
    "    test_dataset = create_dataset_from_dataframe(test_df, tokenizer)\n",
    "    \n",
    "    print(f\"\\n✅ Datasets created successfully:\")\n",
    "    print(f\"   Train: {len(train_dataset):,} samples\")\n",
    "    print(f\"   Val:   {len(val_dataset):,} samples\")\n",
    "    print(f\"   Test:  {len(test_dataset):,} samples\")\n",
    "    \n",
    "    # Verify dataset structure\n",
    "    sample = train_dataset[0]\n",
    "    print(f\"\\n🔍 Dataset structure:\")\n",
    "    print(f\"   input_ids shape: {sample['input_ids'].shape}\")\n",
    "    print(f\"   attention_mask shape: {sample['attention_mask'].shape}\")\n",
    "    print(f\"   labels: {sample['labels'].item()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating datasets: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d09d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6: Initialize Trainer\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"INITIALIZING TRAINER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = MODELS_DIR / \"roberta\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize trainer\n",
    "bert_trainer = BertTrainer(\n",
    "    model_name=ModelConfig.MODEL_NAME,\n",
    "    output_dir=str(output_dir)\n",
    ")\n",
    "\n",
    "print(f\"✅ Trainer initialized:\")\n",
    "print(f\"   Model: {ModelConfig.MODEL_NAME}\")\n",
    "print(f\"   Output: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce12df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: Train Model\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display training configuration\n",
    "print(f\"\\n⚙️  Training Configuration:\")\n",
    "print(f\"   Epochs: {ModelConfig.NUM_EPOCHS}\")\n",
    "print(f\"   Batch size: {ModelConfig.BATCH_SIZE}\")\n",
    "print(f\"   Learning rate: {ModelConfig.LEARNING_RATE}\")\n",
    "print(f\"   Warmup steps: {ModelConfig.WARMUP_STEPS}\")\n",
    "print(f\"   Weight decay: {ModelConfig.WEIGHT_DECAY}\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   FP16: {TrainingConfig.USE_FP16 and torch.cuda.is_available()}\")\n",
    "\n",
    "# Train the model\n",
    "print(f\"\\n🚀 Starting training...\\n\")\n",
    "\n",
    "try:\n",
    "    train_results = bert_trainer.train(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        num_epochs=ModelConfig.NUM_EPOCHS,\n",
    "        batch_size=ModelConfig.BATCH_SIZE,\n",
    "        learning_rate=ModelConfig.LEARNING_RATE,\n",
    "        warmup_steps=ModelConfig.WARMUP_STEPS,\n",
    "        weight_decay=ModelConfig.WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✅ Training completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Training failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display training summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"⏱️  Training time: {train_results['training_time']:.2f}s ({train_results['training_time']/60:.2f}m)\")\n",
    "print(f\"\\n📊 Final validation metrics:\")\n",
    "for k, v in train_results['eval_metrics'].items():\n",
    "    if isinstance(v, (int, float)):\n",
    "        print(f\"   {k:20}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75986bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8: Evaluate on Test Set\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Evaluate\n",
    "    test_results = bert_trainer.evaluate(test_dataset)\n",
    "    \n",
    "    print(\"\\n✅ Test evaluation completed!\")\n",
    "    print(f\"\\n📊 Test metrics:\")\n",
    "    for k, v in test_results.items():\n",
    "        if isinstance(v, (int, float)):\n",
    "            print(f\"   {k:20}: {v:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Evaluation failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75854d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: Get Detailed Predictions\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"GENERATING PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Get predictions\n",
    "    predictions = bert_trainer.trainer.predict(test_dataset)\n",
    "    \n",
    "    # Extract predictions and probabilities\n",
    "    y_test_pred = np.argmax(predictions.predictions, axis=1)\n",
    "    y_test_proba = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
    "    y_test_true = test_df['label'].values\n",
    "    \n",
    "    print(f\"\\n✅ Predictions generated:\")\n",
    "    print(f\"   Samples: {len(y_test_true):,}\")\n",
    "    print(f\"   Predictions shape: {y_test_pred.shape}\")\n",
    "    print(f\"   Probabilities shape: {y_test_proba.shape}\")\n",
    "    \n",
    "    # Prediction distribution\n",
    "    unique, counts = np.unique(y_test_pred, return_counts=True)\n",
    "    print(f\"\\n📊 Prediction distribution:\")\n",
    "    for label, count in zip(unique, counts):\n",
    "        label_name = \"Real\" if label == 0 else \"Fake\"\n",
    "        pct = count / len(y_test_pred) * 100\n",
    "        print(f\"   {label_name}: {count:>6,} ({pct:5.2f}%)\")\n",
    "    \n",
    "    # True label distribution\n",
    "    unique, counts = np.unique(y_test_true, return_counts=True)\n",
    "    print(f\"\\n📊 True label distribution:\")\n",
    "    for label, count in zip(unique, counts):\n",
    "        label_name = \"Real\" if label == 0 else \"Fake\"\n",
    "        pct = count / len(y_test_true) * 100\n",
    "        print(f\"   {label_name}: {count:>6,} ({pct:5.2f}%)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to generate predictions: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995929cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: Compute Extended Metrics\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"COMPUTING EXTENDED METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Compute comprehensive metrics\n",
    "    roberta_evaluation = compute_extended_metrics(\n",
    "        y_true=y_test_true,\n",
    "        y_pred=y_test_pred,\n",
    "        y_proba=y_test_proba\n",
    "    )\n",
    "    \n",
    "    # Add model name\n",
    "    roberta_evaluation['model_name'] = f\"RoBERTa ({ModelConfig.MODEL_NAME})\"\n",
    "    \n",
    "    print(\"\\n✅ Extended metrics computed!\")\n",
    "    print(f\"\\n📊 Complete evaluation results:\")\n",
    "    print(f\"   Accuracy:  {roberta_evaluation['accuracy']:.4f}\")\n",
    "    print(f\"   Precision: {roberta_evaluation['precision']:.4f}\")\n",
    "    print(f\"   Recall:    {roberta_evaluation['recall']:.4f}\")\n",
    "    print(f\"   F1-score:  {roberta_evaluation['f1']:.4f}\")\n",
    "    if 'roc_auc' in roberta_evaluation:\n",
    "        print(f\"   ROC-AUC:   {roberta_evaluation['roc_auc']:.4f}\")\n",
    "    if 'average_precision' in roberta_evaluation:\n",
    "        print(f\"   Avg Precision: {roberta_evaluation['average_precision']:.4f}\")\n",
    "    \n",
    "    # Print classification report\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(f\"\\n📋 Classification Report:\")\n",
    "    print(classification_report(y_test_true, y_test_pred, \n",
    "                                target_names=['Real', 'Fake'], \n",
    "                                digits=4))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to compute metrics: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11: Create Visualizations\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create visualization directory\n",
    "viz_dir = VISUALIZATIONS_DIR / 'roberta'\n",
    "viz_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Confusion Matrix\n",
    "    print(\"\\n📊 Creating confusion matrix...\")\n",
    "    plot_confusion_matrix(\n",
    "        y_true=y_test_true,\n",
    "        y_pred=y_test_pred,\n",
    "        model_name=f\"RoBERTa ({ModelConfig.MODEL_NAME})\",\n",
    "        save_path=viz_dir / \"confusion_matrix.png\"\n",
    "    )\n",
    "    print(f\"   ✅ Saved: {viz_dir / 'confusion_matrix.png'}\")\n",
    "    \n",
    "    # ROC Curve\n",
    "    print(\"\\n📊 Creating ROC curve...\")\n",
    "    plot_roc_curve(\n",
    "        y_true=y_test_true,\n",
    "        y_proba=y_test_proba,\n",
    "        model_name=f\"RoBERTa ({ModelConfig.MODEL_NAME})\",\n",
    "        save_path=viz_dir / \"roc_curve.png\"\n",
    "    )\n",
    "    print(f\"   ✅ Saved: {viz_dir / 'roc_curve.png'}\")\n",
    "    \n",
    "    print(\"\\n✅ All visualizations created!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Visualization failed: {e}\")\n",
    "    # Continue even if visualization fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d93f98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 12: Compare with Baseline\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import json\n",
    "\n",
    "try:\n",
    "    # Try to load baseline results\n",
    "    baseline_path = METRICS_DIR / \"baseline_evaluation_results.json\"\n",
    "    \n",
    "    if baseline_path.exists():\n",
    "        with open(baseline_path, 'r') as f:\n",
    "            baseline_evaluation = json.load(f)\n",
    "        \n",
    "        print(\"✅ Baseline results loaded\")\n",
    "        \n",
    "        # Compare models\n",
    "        print(\"\\n📊 Creating comparison visualization...\")\n",
    "        comparison_df = compare_models(\n",
    "            [baseline_evaluation, roberta_evaluation],\n",
    "            save_path=viz_dir / \"model_comparison.png\"\n",
    "        )\n",
    "        \n",
    "        # Calculate improvements\n",
    "        metrics_to_compare = ['accuracy', 'precision', 'recall', 'f1']\n",
    "        if 'roc_auc' in baseline_evaluation and 'roc_auc' in roberta_evaluation:\n",
    "            metrics_to_compare.append('roc_auc')\n",
    "        \n",
    "        print(f\"\\n🚀 Performance Improvements (RoBERTa vs Baseline):\")\n",
    "        for metric in metrics_to_compare:\n",
    "            if metric in baseline_evaluation and metric in roberta_evaluation:\n",
    "                baseline_val = baseline_evaluation[metric]\n",
    "                roberta_val = roberta_evaluation[metric]\n",
    "                improvement = roberta_val - baseline_val\n",
    "                pct_improvement = (improvement / baseline_val) * 100 if baseline_val > 0 else 0\n",
    "                \n",
    "                print(f\"   {metric.capitalize():15}: {baseline_val:.4f} → {roberta_val:.4f} \"\n",
    "                      f\"(+{improvement:.4f}, +{pct_improvement:.2f}%)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"⚠️  Baseline results not found\")\n",
    "        print(f\"   Looking for: {baseline_path}\")\n",
    "        print(\"   Skipping comparison\")\n",
    "        comparison_df = None\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Comparison failed: {e}\")\n",
    "    comparison_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac05a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13: Save All Results\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create metrics directory\n",
    "metrics_dir = METRICS_DIR / 'roberta'\n",
    "metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # 1. Save evaluation results\n",
    "    eval_path = metrics_dir / \"evaluation_results.json\"\n",
    "    save_evaluation_results(roberta_evaluation, str(eval_path))\n",
    "    print(f\"✅ Evaluation: {eval_path}\")\n",
    "    \n",
    "    # 2. Save training results\n",
    "    train_path = metrics_dir / \"training_results.json\"\n",
    "    save_training_results(train_results, str(train_path))\n",
    "    print(f\"✅ Training:   {train_path}\")\n",
    "    \n",
    "    # 3. Save predictions\n",
    "    predictions_data = {\n",
    "        'y_true': y_test_true.tolist(),\n",
    "        'y_pred': y_test_pred.tolist(),\n",
    "        'y_proba': y_test_proba.tolist(),\n",
    "        'model_name': f\"RoBERTa ({ModelConfig.MODEL_NAME})\",\n",
    "        'num_samples': len(y_test_true),\n",
    "        'num_correct': int((y_test_true == y_test_pred).sum()),\n",
    "        'accuracy': float((y_test_true == y_test_pred).mean())\n",
    "    }\n",
    "    pred_path = metrics_dir / \"test_predictions.json\"\n",
    "    save_json(predictions_data, pred_path)\n",
    "    print(f\"✅ Predictions: {pred_path}\")\n",
    "    \n",
    "    # 4. Save model\n",
    "    bert_trainer.save_model()\n",
    "    print(f\"✅ Model:      {output_dir}\")\n",
    "    \n",
    "    print(\"\\n✅ All results saved successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to save results: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b65a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 14: Final Summary\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎉 ROBERTA MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n✅ Completed Tasks:\")\n",
    "print(\"   ✓ Loaded and prepared data\")\n",
    "print(\"   ✓ Created PyTorch datasets\")\n",
    "print(\"   ✓ Fine-tuned RoBERTa model\")\n",
    "print(\"   ✓ Evaluated on test set\")\n",
    "print(\"   ✓ Generated predictions\")\n",
    "print(\"   ✓ Computed extended metrics\")\n",
    "print(\"   ✓ Created visualizations\")\n",
    "if comparison_df is not None:\n",
    "    print(\"   ✓ Compared with baseline\")\n",
    "print(\"   ✓ Saved all results\")\n",
    "\n",
    "print(f\"\\n📊 Final Model Performance:\")\n",
    "print(f\"   Model:     {ModelConfig.MODEL_NAME}\")\n",
    "print(f\"   Device:    {device}\")\n",
    "print(f\"   Accuracy:  {roberta_evaluation['accuracy']:.4f}\")\n",
    "print(f\"   Precision: {roberta_evaluation['precision']:.4f}\")\n",
    "print(f\"   Recall:    {roberta_evaluation['recall']:.4f}\")\n",
    "print(f\"   F1-score:  {roberta_evaluation['f1']:.4f}\")\n",
    "if 'roc_auc' in roberta_evaluation:\n",
    "    print(f\"   ROC-AUC:   {roberta_evaluation['roc_auc']:.4f}\")\n",
    "print(f\"   Training:  {train_results['training_time']:.2f}s\")\n",
    "\n",
    "print(f\"\\n📁 Output Locations:\")\n",
    "print(f\"   Model:         {output_dir}\")\n",
    "print(f\"   Metrics:       {metrics_dir}\")\n",
    "print(f\"   Visualizations: {viz_dir}\")\n",
    "\n",
    "if comparison_df is not None:\n",
    "    print(f\"\\n🚀 Best Performance Gains:\")\n",
    "    for metric in ['accuracy', 'f1']:\n",
    "        if metric in roberta_evaluation and metric in baseline_evaluation:\n",
    "            improvement = roberta_evaluation[metric] - baseline_evaluation[metric]\n",
    "            print(f\"   {metric.capitalize()}: +{improvement:.4f} (+{improvement*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n🎯 Next Steps:\")\n",
    "print(\"   → Deploy model to production\")\n",
    "print(\"   → Build FastAPI endpoint\")\n",
    "print(\"   → Create frontend interface\")\n",
    "print(\"   → Monitor model performance\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎊 PROJECT READY FOR DEPLOYMENT!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
