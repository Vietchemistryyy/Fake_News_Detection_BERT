{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aac07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL -3: Strip emojis from prints (Kaggle/Colab safe)\n",
    "# ============================================================================\n",
    "import builtins, re\n",
    "\n",
    "_emoji_pattern = re.compile(\n",
    "    \"[\\U0001F600-\\U0001F64F]\"  # emoticons\n",
    "    \"|[\\U0001F300-\\U0001F5FF]\"  # symbols & pictographs\n",
    "    \"|[\\U0001F680-\\U0001F6FF]\"  # transport & map symbols\n",
    "    \"|[\\U0001F1E0-\\U0001F1FF]\"  # flags\n",
    "    \"|[\\u2600-\\u26FF]\"          # misc symbols\n",
    "    \"|[\\u2700-\\u27BF]\"          # dingbats\n",
    "    , flags=re.UNICODE\n",
    ")\n",
    "\n",
    "_orig_print = builtins.print\n",
    "\n",
    "def _clean_text(obj):\n",
    "    try:\n",
    "        s = str(obj)\n",
    "        s = _emoji_pattern.sub('', s)\n",
    "        return s\n",
    "    except Exception:\n",
    "        return str(obj)\n",
    "\n",
    "def print(*args, **kwargs):\n",
    "    cleaned = [_clean_text(a) for a in args]\n",
    "    return _orig_print(*cleaned, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbd1587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL -2: Kaggle/Colab Dependency Setup\n",
    "# ============================================================================\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "packages = [\n",
    "    'transformers',\n",
    "    'datasets',\n",
    "    'accelerate',\n",
    "    'scikit-learn',\n",
    "    'pandas',\n",
    "    'matplotlib',\n",
    "    'seaborn'\n",
    "]\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    for p in pkgs:\n",
    "        try:\n",
    "            __import__(p.replace('-', '_'))\n",
    "        except Exception:\n",
    "            print(f\"Installing {p}...\")\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', p])\n",
    "\n",
    "# Try requirements.txt first if present, else fallback to individual installs\n",
    "try:\n",
    "    import os\n",
    "    if os.path.exists('requirements.txt'):\n",
    "        print('Installing from requirements.txt ...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', '-r', 'requirements.txt'])\n",
    "    else:\n",
    "        pip_install(packages)\n",
    "except Exception as e:\n",
    "    print(f\"requirements.txt install failed: {e}; installing core packages...\")\n",
    "    pip_install(packages)\n",
    "\n",
    "print('✅ Dependencies ready')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74e90ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING ENVIRONMENT SETUP\n",
      "================================================================================\n",
      "✅ Path setup successful\n",
      "✅ Transformers version: 4.39.3\n",
      "✅ AutoTokenizer and AutoModel imports successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5760\\4038866280.py\", line 27, in <module>\n",
      "    from transformers import Trainer\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1462, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1472, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\transformers\\trainer.py\", line 41, in <module>\n",
      "    from .integrations import (\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1462, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1472, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py\", line 72, in <module>\n",
      "    from ..trainer_callback import ProgressCallback, TrainerCallback  # noqa: E402\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\transformers\\trainer_callback.py\", line 27, in <module>\n",
      "    from .training_args import TrainingArguments\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\transformers\\training_args.py\", line 72, in <module>\n",
      "    from accelerate.state import AcceleratorState, PartialState\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\accelerate\\__init__.py\", line 16, in <module>\n",
      "    from .accelerator import Accelerator\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\accelerate\\accelerator.py\", line 35, in <module>\n",
      "    from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\accelerate\\checkpointing.py\", line 24, in <module>\n",
      "    from .utils import (\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\accelerate\\utils\\__init__.py\", line 178, in <module>\n",
      "    from .fsdp_utils import load_fsdp_model, load_fsdp_optimizer, save_fsdp_model, save_fsdp_optimizer\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\accelerate\\utils\\fsdp_utils.py\", line 26, in <module>\n",
      "    import torch.distributed.checkpoint as dist_cp\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\distributed\\checkpoint\\__init__.py\", line 2, in <module>\n",
      "    from .default_planner import DefaultLoadPlanner, DefaultSavePlanner\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\distributed\\checkpoint\\default_planner.py\", line 13, in <module>\n",
      "    from torch.distributed._tensor import DTensor\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\distributed\\_tensor\\__init__.py\", line 6, in <module>\n",
      "    import torch.distributed._tensor.ops\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\distributed\\_tensor\\ops\\__init__.py\", line 2, in <module>\n",
      "    from .embedding_ops import *  # noqa: F403\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\distributed\\_tensor\\ops\\embedding_ops.py\", line 8, in <module>\n",
      "    import torch.distributed._functional_collectives as funcol\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\distributed\\_functional_collectives.py\", line 12, in <module>\n",
      "    from . import _functional_collectives_impl as fun_col_impl\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\distributed\\_functional_collectives_impl.py\", line 36, in <module>\n",
      "    from torch._dynamo import assume_constant_result\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\_dynamo\\__init__.py\", line 64, in <module>\n",
      "    torch.manual_seed = disable(torch.manual_seed)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\_dynamo\\decorators.py\", line 50, in disable\n",
      "    return DisableContext()(fn)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 410, in __call__\n",
      "    (filename is None or trace_rules.check(fn))\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 3378, in check\n",
      "    return check_verbose(obj, is_inlined_call).skipped\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 3361, in check_verbose\n",
      "    rule = torch._dynamo.trace_rules.lookup_inner(\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 3442, in lookup_inner\n",
      "    rule = get_torch_obj_rule_map().get(obj, None)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 2782, in get_torch_obj_rule_map\n",
      "    obj = load_object(k)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 2811, in load_object\n",
      "    val = _load_obj_from_str(x[0])\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 2795, in _load_obj_from_str\n",
      "    return getattr(importlib.import_module(module), obj_name)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\nested\\_internal\\nested_tensor.py\", line 417, in <module>\n",
      "    values=torch.randn(3, 3, device=\"meta\"),\n",
      "d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\torch\\nested\\_internal\\nested_tensor.py:417: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  values=torch.randn(3, 3, device=\"meta\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Trainer import successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Fake_News_Detection_BERT\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenizer creation successful\n",
      "✅ Transformers 4.39.3 loaded successfully\n",
      "✅ Config import successful\n",
      "✅ TRANSFORMERS_AVAILABLE: True\n",
      "\n",
      "🎉 Environment setup complete! Ready for BERT training.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 0: Test Environment Setup\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING ENVIRONMENT SETUP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test basic imports\n",
    "try:\n",
    "    import sys\n",
    "    sys.path.append('..')\n",
    "    print(\"✅ Path setup successful\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Path setup failed: {e}\")\n",
    "\n",
    "# Test transformers\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"✅ Transformers version: {transformers.__version__}\")\n",
    "    \n",
    "    # Test specific imports\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    print(\"✅ AutoTokenizer and AutoModel imports successful\")\n",
    "    \n",
    "    # Try to import Trainer separately\n",
    "    try:\n",
    "        from transformers import Trainer\n",
    "        print(\"✅ Trainer import successful\")\n",
    "    except ImportError:\n",
    "        print(\"⚠️ Trainer not available in this transformers version\")\n",
    "        print(\"💡 Trying alternative import...\")\n",
    "        from transformers.trainer import Trainer\n",
    "        print(\"✅ Trainer import successful (alternative)\")\n",
    "    \n",
    "    # Test tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    print(\"✅ Tokenizer creation successful\")\n",
    "    \n",
    "    # Test our modules\n",
    "    from src.config import ModelConfig, DataConfig\n",
    "    print(\"✅ Config import successful\")\n",
    "    \n",
    "    from src.train import TRANSFORMERS_AVAILABLE\n",
    "    print(f\"✅ TRANSFORMERS_AVAILABLE: {TRANSFORMERS_AVAILABLE}\")\n",
    "    \n",
    "    print(\"\\n🎉 Environment setup complete! Ready for BERT training.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Environment setup failed: {e}\")\n",
    "    print(\"💡 Please check your Python environment and dependencies\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09732c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL -0: Ensure Project Directories\n",
    "# ============================================================================\n",
    "from src.config import create_directories\n",
    "create_directories()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1765a464",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# NOTEBOOK 04: BERT MODEL TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "## 🎯 Objective\n",
    "Fine-tune a BERT model for fake news detection and compare performance with the baseline model.\n",
    "\n",
    "## 📋 What we'll do:\n",
    "1. **Load preprocessed data** from notebook 02\n",
    "2. **Prepare PyTorch datasets** for BERT training\n",
    "3. **Fine-tune BERT model** using Hugging Face Transformers\n",
    "4. **Evaluate performance** on train/val/test sets\n",
    "5. **Compare with baseline** model performance\n",
    "6. **Save model** and results\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c455185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports successful!\n",
      "🖥️  Device: cpu\n",
      "\n",
      "🔍 Testing transformers availability...\n",
      "✅ Transformers imports successful!\n",
      "✅ Tokenizer test successful!\n",
      "📊 Ready to train BERT model!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: Imports and Setup\n",
    "# ============================================================================\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import from src\n",
    "from src.config import (\n",
    "    DataConfig, ModelConfig, TrainingConfig, \n",
    "    PROCESSED_DATA_DIR, METRICS_DIR, VISUALIZATIONS_DIR, MODELS_DIR\n",
    ")\n",
    "from src.dataset import create_data_loaders\n",
    "from src.train import BertTrainer, train_bert_model\n",
    "from src.evaluate import (\n",
    "    evaluate_model, \n",
    "    plot_confusion_matrix, \n",
    "    plot_roc_curve,\n",
    "    compare_models,\n",
    "    save_evaluation_results\n",
    ")\n",
    "from src.utils import save_json\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"✅ Imports successful!\")\n",
    "print(f\"🖥️  Device: {device}\")\n",
    "\n",
    "# Test transformers availability\n",
    "print(f\"\\n🔍 Testing transformers availability...\")\n",
    "try:\n",
    "    from transformers import Trainer, AutoTokenizer, AutoModelForSequenceClassification\n",
    "    print(\"✅ Transformers imports successful!\")\n",
    "    \n",
    "    # Test tokenizer\n",
    "    test_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    print(\"✅ Tokenizer test successful!\")\n",
    "    \n",
    "    print(f\"📊 Ready to train BERT model!\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Transformers not available: {e}\")\n",
    "    print(\"💡 Please install: pip install transformers\")\n",
    "    print(\"⚠️  Only baseline model will be available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d8ff40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING DATA AND PREPARING TOKENIZER\n",
      "================================================================================\n",
      "\n",
      "📊 Data loaded successfully!\n",
      "   Train set: 95,244 samples\n",
      "   Val set:   20,409 samples\n",
      "   Test set:  20,410 samples\n",
      "\n",
      "🔤 Loading tokenizer: roberta-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022ba46c4f7a4a4dacc1a4bf27abe2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81b88aa03514a4bbf27e5f0fed7dd15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3418c7f631cf408eacd1a903f1e7e538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f59177619541d5aed920befeb59d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea05d91fba75423a870e4306467aa1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Tokenizer info:\n",
      "   Vocab size: 50,265\n",
      "   Max length: 256\n",
      "   Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}\n",
      "\n",
      "🧪 Sample tokenization:\n",
      "   Original text length: 85 chars\n",
      "   Tokenized length: 26 tokens\n",
      "   Sample tokens: [0, 642, 718, 5992, 32, 6749, 154, 31, 821, 7043]...\n",
      "\n",
      "✅ Import verification:\n",
      "   MODELS_DIR: d:\\Fake_News_Detection_BERT\\notebooks\\..\\models\n",
      "   METRICS_DIR: d:\\Fake_News_Detection_BERT\\notebooks\\..\\results\\metrics\n",
      "   VISUALIZATIONS_DIR: d:\\Fake_News_Detection_BERT\\notebooks\\..\\results\\visualizations\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Load Data and Prepare Tokenizer\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING DATA AND PREPARING TOKENIZER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the processed datasets\n",
    "train_df = pd.read_csv(DataConfig.TRAIN_PATH)\n",
    "val_df = pd.read_csv(DataConfig.VAL_PATH)\n",
    "test_df = pd.read_csv(DataConfig.TEST_PATH)\n",
    "\n",
    "print(f\"\\n📊 Data loaded successfully!\")\n",
    "print(f\"   Train set: {train_df.shape[0]:,} samples\")\n",
    "print(f\"   Val set:   {val_df.shape[0]:,} samples\")\n",
    "print(f\"   Test set:  {test_df.shape[0]:,} samples\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"\\n🔤 Loading tokenizer: {ModelConfig.MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "\n",
    "print(f\"\\n📋 Tokenizer info:\")\n",
    "print(f\"   Vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"   Max length: {ModelConfig.MAX_LENGTH}\")\n",
    "print(f\"   Special tokens: {tokenizer.special_tokens_map}\")\n",
    "\n",
    "# Test tokenization on sample text\n",
    "sample_text = train_df.iloc[0]['cleaned_content']\n",
    "sample_tokens = tokenizer.encode(sample_text, max_length=ModelConfig.MAX_LENGTH, truncation=True)\n",
    "print(f\"\\n🧪 Sample tokenization:\")\n",
    "print(f\"   Original text length: {len(sample_text)} chars\")\n",
    "print(f\"   Tokenized length: {len(sample_tokens)} tokens\")\n",
    "print(f\"   Sample tokens: {sample_tokens[:10]}...\")\n",
    "\n",
    "# Verify imports\n",
    "print(f\"\\n✅ Import verification:\")\n",
    "print(f\"   MODELS_DIR: {MODELS_DIR}\")\n",
    "print(f\"   METRICS_DIR: {METRICS_DIR}\")\n",
    "print(f\"   VISUALIZATIONS_DIR: {VISUALIZATIONS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e62c6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.dataset:Dataset initialized with 95244 samples\n",
      "INFO:src.dataset:Max length: 256\n",
      "INFO:src.dataset:Dataset initialized with 20409 samples\n",
      "INFO:src.dataset:Max length: 256\n",
      "INFO:src.dataset:Dataset initialized with 20410 samples\n",
      "INFO:src.dataset:Max length: 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING PYTORCH DATASETS\n",
      "================================================================================\n",
      "\n",
      "📊 Datasets created:\n",
      "   Train dataset: 95244 samples\n",
      "   Val dataset:   20409 samples\n",
      "   Test dataset:  20410 samples\n",
      "\n",
      "🧪 Sample batch info:\n",
      "   Input IDs shape: torch.Size([1, 256])\n",
      "   Attention mask shape: torch.Size([1, 256])\n",
      "   Labels: 1\n",
      "\n",
      "📝 Sample decoded text (first 200 chars):\n",
      "   pilots are resigning from german air force – they don ’ t want to fight against russia....\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Create PyTorch Datasets\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING PYTORCH DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from src.dataset import create_dataset_from_dataframe\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = create_dataset_from_dataframe(train_df, tokenizer)\n",
    "val_dataset = create_dataset_from_dataframe(val_df, tokenizer)\n",
    "test_dataset = create_dataset_from_dataframe(test_df, tokenizer)\n",
    "\n",
    "print(f\"\\n📊 Datasets created:\")\n",
    "print(f\"   Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"   Val dataset:   {len(val_dataset)} samples\")\n",
    "print(f\"   Test dataset:  {len(test_dataset)} samples\")\n",
    "\n",
    "# Test dataset sample\n",
    "sample_item = train_dataset[0]\n",
    "sample_batch = {\n",
    "    'input_ids': sample_item['input_ids'].unsqueeze(0),  # Add batch dimension\n",
    "    'attention_mask': sample_item['attention_mask'].unsqueeze(0),\n",
    "    'labels': sample_item['labels'].unsqueeze(0)\n",
    "}\n",
    "\n",
    "print(f\"\\n🧪 Sample batch info:\")\n",
    "print(f\"   Input IDs shape: {sample_batch['input_ids'].shape}\")\n",
    "print(f\"   Attention mask shape: {sample_batch['attention_mask'].shape}\")\n",
    "print(f\"   Labels: {sample_batch['labels'].item()}\")\n",
    "\n",
    "# Decode sample tokens\n",
    "sample_input_ids = sample_item['input_ids']\n",
    "sample_decoded = tokenizer.decode(sample_input_ids, skip_special_tokens=True)\n",
    "print(f\"\\n📝 Sample decoded text (first 200 chars):\")\n",
    "print(f\"   {sample_decoded[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24edd8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING BERT MODEL\n",
      "================================================================================\n",
      "🔍 Checking transformers availability...\n",
      "❌ Transformers import failed: cannot import name 'HF_DATASETS_DISABLE_PROGRESS_BARS' from 'datasets.config' (d:\\Fake_News_Detection_BERT\\.venv\\Lib\\site-packages\\datasets\\config.py)\n",
      "💡 Please install transformers: pip install transformers\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TRANSFORMERS_AVAILABLE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m💡 Please install transformers: pip install transformers\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Create BERT trainer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mTRANSFORMERS_AVAILABLE\u001b[49m:\n\u001b[32m     32\u001b[39m     bert_trainer = BertTrainer(\n\u001b[32m     33\u001b[39m         model_name=ModelConfig.MODEL_NAME,\n\u001b[32m     34\u001b[39m         output_dir=MODELS_DIR / \u001b[33m\"\u001b[39m\u001b[33mbert\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m     )\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ BERT trainer created successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'TRANSFORMERS_AVAILABLE' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Train BERT Model\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING BERT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import MODELS_DIR if not already imported\n",
    "from src.config import MODELS_DIR\n",
    "\n",
    "# Debug transformers availability\n",
    "print(\"🔍 Checking transformers availability...\")\n",
    "try:\n",
    "    from transformers import Trainer, AutoTokenizer, AutoModelForSequenceClassification\n",
    "    print(\"✅ Transformers imports successful!\")\n",
    "    \n",
    "    # Check TRANSFORMERS_AVAILABLE flag\n",
    "    from src.train import TRANSFORMERS_AVAILABLE\n",
    "    print(f\"📊 TRANSFORMERS_AVAILABLE flag: {TRANSFORMERS_AVAILABLE}\")\n",
    "    \n",
    "    if not TRANSFORMERS_AVAILABLE:\n",
    "        print(\"⚠️ TRANSFORMERS_AVAILABLE is False, trying to fix...\")\n",
    "        import transformers\n",
    "        print(f\"✅ Transformers version: {transformers.__version__}\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Transformers import failed: {e}\")\n",
    "    print(\"💡 Please install transformers: pip install transformers\")\n",
    "\n",
    "# Create BERT trainer\n",
    "if TRANSFORMERS_AVAILABLE:\n",
    "    bert_trainer = BertTrainer(\n",
    "        model_name=ModelConfig.MODEL_NAME,\n",
    "        output_dir=MODELS_DIR / \"roberta\"\n",
    "    )\n",
    "    print(\"✅ RoBERTa trainer created successfully!\")\n",
    "else:\n",
    "    print(\"❌ Cannot create BERT trainer - transformers not available\")\n",
    "    bert_trainer = None\n",
    "\n",
    "# Train the model\n",
    "if bert_trainer is not None:\n",
    "    print(f\"\\n🚀 Starting RoBERTa training...\")\n",
    "    print(f\"   Model: {ModelConfig.MODEL_NAME}\")\n",
    "    print(f\"   Epochs: {ModelConfig.NUM_EPOCHS}\")\n",
    "    print(f\"   Batch size: {ModelConfig.BATCH_SIZE}\")\n",
    "    print(f\"   Learning rate: {ModelConfig.LEARNING_RATE}\")\n",
    "    print(f\"   Max length: {ModelConfig.MAX_LENGTH}\")\n",
    "\n",
    "    train_results = bert_trainer.train(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        num_epochs=ModelConfig.NUM_EPOCHS,\n",
    "        batch_size=ModelConfig.BATCH_SIZE,\n",
    "        learning_rate=ModelConfig.LEARNING_RATE,\n",
    "        warmup_steps=ModelConfig.WARMUP_STEPS,\n",
    "        weight_decay=ModelConfig.WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "    print(\"\\n✅ RoBERTa model training completed!\")\n",
    "    print(f\"\\n📊 Training Results Summary:\")\n",
    "    print(f\"   Training time: {train_results['training_time']:.2f} seconds\")\n",
    "    print(f\"   Final validation metrics: {train_results['eval_metrics']}\")\n",
    "else:\n",
    "    print(\"\\n❌ Cannot start BERT training - transformers not available\")\n",
    "    print(\"💡 Please install transformers: pip install transformers\")\n",
    "    train_results = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d09d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Evaluate on Test Set\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import MODELS_DIR if not already imported\n",
    "from src.config import MODELS_DIR\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = bert_trainer.evaluate(test_dataset)\n",
    "\n",
    "print(\"\\n✅ Test evaluation completed!\")\n",
    "print(f\"\\n📊 Test Results (RoBERTa):\")\n",
    "for metric, value in test_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   {metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "bert_trainer.save_model()\n",
    "print(f\"\\n💾 Model saved to: {MODELS_DIR / 'roberta'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce12df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Get Predictions and Probabilities\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"GETTING PREDICTIONS AND PROBABILITIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get predictions on test set\n",
    "predictions = bert_trainer.trainer.predict(test_dataset)\n",
    "y_test_pred = np.argmax(predictions.predictions, axis=1)\n",
    "y_test_proba = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
    "\n",
    "# Get true labels\n",
    "y_test_true = test_df['label'].values\n",
    "\n",
    "print(f\"\\n📊 Predictions generated:\")\n",
    "print(f\"   Test samples: {len(y_test_true)}\")\n",
    "print(f\"   Predictions shape: {y_test_pred.shape}\")\n",
    "print(f\"   Probabilities shape: {y_test_proba.shape}\")\n",
    "\n",
    "# Show prediction distribution\n",
    "unique, counts = np.unique(y_test_pred, return_counts=True)\n",
    "print(f\"\\n📈 Prediction distribution:\")\n",
    "for label, count in zip(unique, counts):\n",
    "    label_name = \"Real\" if label == 0 else \"Fake\"\n",
    "    percentage = count / len(y_test_pred) * 100\n",
    "    print(f\"   {label_name}: {count:,} ({percentage:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75986bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: Comprehensive Evaluation\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Comprehensive evaluation\n",
    "bert_evaluation = evaluate_model(\n",
    "    y_true=y_test_true,\n",
    "    y_pred=y_test_pred,\n",
    "    y_proba=y_test_proba,\n",
    "    model_name=f\"RoBERTa ({ModelConfig.MODEL_NAME})\"\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Detailed RoBERTa Test Results:\")\n",
    "for metric, value in bert_evaluation.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   {metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(f\"\\n📋 Classification Report:\")\n",
    "print(classification_report(y_test_true, y_test_pred, target_names=['Real', 'Fake']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75854d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: Visualizations\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    y_true=y_test_true,\n",
    "    y_pred=y_test_pred,\n",
    "    model_name=f\"RoBERTa ({ModelConfig.MODEL_NAME})\",\n",
    "    save_path=VISUALIZATIONS_DIR / \"roberta_confusion_matrix.png\"\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Confusion matrix saved!\")\n",
    "\n",
    "# Create ROC curve\n",
    "plot_roc_curve(\n",
    "    y_true=y_test_true,\n",
    "    y_proba=y_test_proba,\n",
    "    model_name=f\"RoBERTa ({ModelConfig.MODEL_NAME})\",\n",
    "    save_path=VISUALIZATIONS_DIR / \"roberta_roc_curve.png\"\n",
    ")\n",
    "\n",
    "print(\"✅ ROC curve saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995929cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: Compare with Baseline Model\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARING WITH BASELINE MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load baseline results\n",
    "import json\n",
    "try:\n",
    "    with open(METRICS_DIR / \"baseline_evaluation_results.json\", 'r') as f:\n",
    "        baseline_evaluation = json.load(f)\n",
    "    \n",
    "    print(\"\\n📊 Loading baseline results for comparison...\")\n",
    "    \n",
    "    # Compare models\n",
    "    comparison_df = compare_models(\n",
    "        [baseline_evaluation, bert_evaluation],\n",
    "        save_path=VISUALIZATIONS_DIR / \"model_comparison.png\"\n",
    "    )\n",
    "    \n",
    "    # Calculate improvements\n",
    "    accuracy_improvement = bert_evaluation['accuracy'] - baseline_evaluation['accuracy']\n",
    "    f1_improvement = bert_evaluation['f1'] - baseline_evaluation['f1']\n",
    "    roc_auc_improvement = bert_evaluation.get('roc_auc', 0) - baseline_evaluation.get('roc_auc', 0)\n",
    "    \n",
    "    print(f\"\\n🚀 RoBERTa vs Baseline Improvements:\")\n",
    "    print(f\"   Accuracy: +{accuracy_improvement:.4f} ({accuracy_improvement*100:.2f}%)\")\n",
    "    print(f\"   F1-score: +{f1_improvement:.4f} ({f1_improvement*100:.2f}%)\")\n",
    "    print(f\"   ROC-AUC:  +{roc_auc_improvement:.4f} ({roc_auc_improvement*100:.2f}%)\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"⚠️  Baseline results not found. Run notebook 03 first to compare models.\")\n",
    "    comparison_df = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: Save Results and Model\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING RESULTS AND MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import MODELS_DIR if not already imported\n",
    "from src.config import MODELS_DIR\n",
    "\n",
    "# Save RoBERTa evaluation results\n",
    "save_evaluation_results(\n",
    "    bert_evaluation,\n",
    "    METRICS_DIR / \"roberta_evaluation_results.json\"\n",
    ")\n",
    "\n",
    "# Save training results\n",
    "save_json(\n",
    "    train_results,\n",
    "    METRICS_DIR / \"roberta_training_results.json\"\n",
    ")\n",
    "\n",
    "# Save test predictions\n",
    "predictions_data = {\n",
    "    'y_true': y_test_true.tolist(),\n",
    "    'y_pred': y_test_pred.tolist(),\n",
    "    'y_proba': y_test_proba.tolist(),\n",
    "    'model_name': f\"RoBERTa ({ModelConfig.MODEL_NAME})\",\n",
    "    'test_samples': len(y_test_true)\n",
    "}\n",
    "\n",
    "save_json(\n",
    "    predictions_data,\n",
    "    METRICS_DIR / \"roberta_test_predictions.json\"\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Files saved:\")\n",
    "print(\"   📊 RoBERTa evaluation: \" + str(METRICS_DIR / 'roberta_evaluation_results.json'))\n",
    "print(f\"   📈 Training results:   {METRICS_DIR / 'roberta_training_results.json'}\")\n",
    "print(f\"   🎯 Test predictions:   {METRICS_DIR / 'roberta_test_predictions.json'}\")\n",
    "print(f\"   🤖 Model directory:    {MODELS_DIR / 'roberta'}\")\n",
    "\n",
    "# Verify saved files\n",
    "import os\n",
    "print(f\"\\n🔍 Verifying saved files:\")\n",
    "for file_path in [\n",
    "    METRICS_DIR / \"roberta_evaluation_results.json\",\n",
    "    METRICS_DIR / \"roberta_training_results.json\",\n",
    "    METRICS_DIR / \"roberta_test_predictions.json\",\n",
    "    MODELS_DIR / \"roberta\"\n",
    "]:\n",
    "    if os.path.exists(file_path):\n",
    "        if os.path.isfile(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"   ✓ {file_path} ({size_mb:.2f} MB)\")\n",
    "        else:\n",
    "            print(f\"   ✓ {file_path} (directory)\")\n",
    "    else:\n",
    "        print(f\"   ✗ {file_path} (not found)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d93f98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: Final Summary\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"BERT MODEL TRAINING COMPLETE! ✅\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📌 What we accomplished:\")\n",
    "print(\"   ✓ Fine-tuned BERT model for fake news detection\")\n",
    "print(\"   ✓ Achieved excellent performance on test set\")\n",
    "print(\"   ✓ Created comprehensive visualizations\")\n",
    "print(\"   ✓ Compared with baseline model performance\")\n",
    "print(\"   ✓ Saved model and all results\")\n",
    "\n",
    "print(f\"\\n🎯 BERT Model Performance:\")\n",
    "print(f\"   📊 Test Accuracy: {bert_evaluation['accuracy']:.4f}\")\n",
    "print(f\"   📊 Test F1-score: {bert_evaluation['f1']:.4f}\")\n",
    "print(f\"   📊 Test ROC-AUC:  {bert_evaluation.get('roc_auc', 0):.4f}\")\n",
    "print(f\"   ⏱️  Training time: {train_results['training_time']:.2f} seconds\")\n",
    "\n",
    "if comparison_df is not None:\n",
    "    print(f\"\\n🚀 Performance Improvements over Baseline:\")\n",
    "    accuracy_improvement = bert_evaluation['accuracy'] - baseline_evaluation['accuracy']\n",
    "    f1_improvement = bert_evaluation['f1'] - baseline_evaluation['f1']\n",
    "    print(f\"   📈 Accuracy: +{accuracy_improvement:.4f} ({accuracy_improvement*100:.2f}%)\")\n",
    "    print(f\"   📈 F1-score: +{f1_improvement:.4f} ({f1_improvement*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n🎉 Project Status:\")\n",
    "print(\"   ✅ Data preprocessing completed (Notebook 02)\")\n",
    "print(\"   ✅ Baseline model trained (Notebook 03)\")\n",
    "print(\"   ✅ BERT model trained (Notebook 04)\")\n",
    "print(\"   🎯 Ready for deployment and API development!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac05a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b65a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3994955f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6fc96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e03a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13e056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25f4883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c92625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9103a177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f720c157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462468a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6633a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 12: Evaluate on Your Own CSV (Real-world Data)\n",
    "# ============================================================================\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.config import PREDICTIONS_DIR, DataConfig\n",
    "from src.evaluate import compute_extended_metrics, plot_confusion_matrix, plot_roc_curve\n",
    "\n",
    "# How to use:\n",
    "# 1) Kaggle: Add your CSV as an input dataset or use the file navigator to upload.\n",
    "# 2) Colab: Set csv_path to a file in /content or use files.upload().\n",
    "# 3) Local: Set csv_path to your local file path.\n",
    "\n",
    "# Configure your CSV path here (string). Leave empty to use uploader on Colab.\n",
    "csv_path = \"\"  # e.g., \"/kaggle/input/mydata/eval.csv\" or \"/content/eval.csv\"\n",
    "\n",
    "# Try Colab uploader if no path supplied\n",
    "if not csv_path:\n",
    "    try:\n",
    "        from google.colab import files  # type: ignore\n",
    "        print(\"No csv_path set. Use the dialog to upload a CSV...\")\n",
    "        uploaded = files.upload()\n",
    "        if uploaded:\n",
    "            csv_path = list(uploaded.keys())[0]\n",
    "            print(f\"✅ Uploaded: {csv_path}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "assert csv_path, \"Please set csv_path to your CSV file or upload one.\"\n",
    "\n",
    "# Load CSV\n",
    "user_df = pd.read_csv(csv_path)\n",
    "print(f\"Loaded CSV with shape: {user_df.shape}\")\n",
    "print(\"Columns:\", list(user_df.columns))\n",
    "\n",
    "# Detect text column\n",
    "text_col_candidates = [\n",
    "    DataConfig.CLEANED_TEXT_COLUMN,\n",
    "    DataConfig.TEXT_COLUMN,\n",
    "    'text', 'content', 'clean_text', 'cleaned_text'\n",
    "]\n",
    "text_col = None\n",
    "for c in text_col_candidates:\n",
    "    if c in user_df.columns:\n",
    "        text_col = c\n",
    "        break\n",
    "assert text_col is not None, f\"Could not find a text column. Please include one of: {text_col_candidates}\"\n",
    "\n",
    "# Optional label column\n",
    "label_col = DataConfig.LABEL_COLUMN if DataConfig.LABEL_COLUMN in user_df.columns else None\n",
    "\n",
    "# Build a lightweight dataset for inference\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=ModelConfig.MAX_LENGTH):\n",
    "        self.texts = list(map(str, texts))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=ModelConfig.TRUNCATION,\n",
    "            padding=ModelConfig.PADDING,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "# Use tokenizer from training section\n",
    "assert tokenizer is not None, \"Tokenizer not available. Please run previous cells to load the model/tokenizer.\"\n",
    "\n",
    "infer_ds = InferenceDataset(user_df[text_col].values, tokenizer)\n",
    "\n",
    "# Predict\n",
    "pred_out = bert_trainer.trainer.predict(infer_ds)\n",
    "preds = np.argmax(pred_out.predictions, axis=1)\n",
    "proba = torch.softmax(torch.tensor(pred_out.predictions), dim=1).numpy()\n",
    "\n",
    "# Save predictions CSV\n",
    "custom_pred_dir = Path(PREDICTIONS_DIR) / 'roberta'\n",
    "custom_pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "custom_csv = custom_pred_dir / 'custom_predictions.csv'\n",
    "\n",
    "save_cols = {\n",
    "    'text': user_df[text_col].tolist(),\n",
    "    'pred': preds.tolist(),\n",
    "}\n",
    "# Include probabilities\n",
    "if proba.ndim == 2 and proba.shape[1] == 2:\n",
    "    save_cols['proba_real'] = proba[:, 0].tolist()\n",
    "    save_cols['proba_fake'] = proba[:, 1].tolist()\n",
    "\n",
    "# Include labels if present\n",
    "if label_col is not None:\n",
    "    save_cols['label'] = user_df[label_col].tolist()\n",
    "\n",
    "pd.DataFrame(save_cols).to_csv(custom_csv, index=False)\n",
    "print(f\"💾 Saved custom predictions to: {custom_csv}\")\n",
    "\n",
    "# If labels exist, compute extended metrics and plots\n",
    "if label_col is not None:\n",
    "    y_true = user_df[label_col].values\n",
    "    y_pred = preds\n",
    "    y_proba = proba\n",
    "    ext = compute_extended_metrics(y_true, y_pred, y_proba)\n",
    "    custom_metrics_json = Path(METRICS_DIR) / 'roberta' / 'custom_extended_metrics.json'\n",
    "    custom_metrics_json.parent.mkdir(parents=True, exist_ok=True)\n",
    "    save_evaluation_results(ext, custom_metrics_json)\n",
    "    print(f\"✅ Saved custom extended metrics to: {custom_metrics_json}\")\n",
    "\n",
    "    # Plots\n",
    "    plot_confusion_matrix(y_true, y_pred, model_name=f\"RoBERTa ({ModelConfig.MODEL_NAME})\",\n",
    "                          save_path=Path(VISUALIZATIONS_DIR) / 'roberta' / 'custom_confusion_matrix.png')\n",
    "    if y_proba is not None:\n",
    "        plot_roc_curve(y_true, y_proba, model_name=f\"RoBERTa ({ModelConfig.MODEL_NAME})\",\n",
    "                       save_path=Path(VISUALIZATIONS_DIR) / 'roberta' / 'custom_roc_curve.png')\n",
    "else:\n",
    "    print(\"No label column found; generated predictions only.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
